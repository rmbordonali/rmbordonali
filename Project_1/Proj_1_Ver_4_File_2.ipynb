{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36d171d6-fd92-4076-ae50-40ffef4b34d3",
   "metadata": {},
   "source": [
    "# Water Level Data\r\n",
    "\r\n",
    "### Code Summary:\r\n",
    "\r\n",
    "- Detrending, normalizing, and shifting data to zero mean\r\n",
    "- Fast Fourier transform magnitude and phase spectra plots\r\n",
    "- Continuous wavelet transform scalograms with Morlet wavelet\r\n",
    "- Subsampled data plots\r\n",
    "- Tapered data plots\r\n",
    "- Histograms and statistics for plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adcc62d5-0f87-4c87-9105-3616b8917351",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data source: https://www.isdm-gdsi.gc.ca/isdm-gdsi/twl-mne/maps-cartes/inventory-inventaire-eng.asp?user=isdm-gdsi&region=MEDS&tst=1&perm=0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fb179cd-5800-4349-9f02-5ef1a8c361c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.fft\n",
    "import pywt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.tools as tls\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal.windows import hann\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96840545-7e4e-4235-b6f3-95a3ff06f932",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Toronto-daily-mean-water-1960-2024.csv\", index_col=\"Obs_date\")\n",
    "\n",
    "# Check for null values in dataframe:\n",
    "any_null = df1.isnull().any().any()\n",
    "print(any_null) # Will show false if no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87b1ea0d-9a22-4163-a35c-148f053803f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca4eb78-1f71-4fa0-9f63-8925e52eb6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearly detrend data, normalize and shift to zero mean:\n",
    "df1['Detrended'] = signal.detrend(df1['SLEV(metres)'], type='linear')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df1['Normalized'] = scaler.fit_transform(df1['Detrended'].values.reshape(-1, 1)).flatten()\n",
    "print(f\"Min-Max normalized range: [{df1['Normalized'].min():.3f}, {df1['Normalized'].max():.3f}]\")\n",
    "\n",
    "# Shift Mean:\n",
    "df1['Normalized_Shifted'] = (df1['Normalized'] - 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6034f8c9-4437-407e-a335-93946806e62b",
   "metadata": {},
   "source": [
    "***\n",
    "### Fast Fourier Transform (Subsampled and Tapered Data)\n",
    "#### Notes:\n",
    "- Applying the fast fourier transform to subsampled and tapered DNS data to plot the resulting magnitude and phase spectra\n",
    "<br> Effects of subsampling/tapering:\n",
    "    - removes higher frequency content\n",
    "    - reduces spectral leakage caused by discontinuities\n",
    "<br>\n",
    "- Change plot x-limits to zoom in on points of interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd78a3a-e34c-4002-86a0-702d7439c103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous subsampling and tapering related code, determined that step = 52 goes well with percent_check,\n",
    "# and taper with no padding prevents discontinuities\n",
    "sub_step = 52\n",
    "\n",
    "L = len(df1['Normalized_Shifted'])\n",
    "data = df1['Normalized_Shifted'].values\n",
    "time_index = pd.to_datetime(df1.index)\n",
    "# time delta (1 day for water level data)\n",
    "time_delta = (time_index[1] - time_index[0])\n",
    "\n",
    "window = hann(L)\n",
    "tapered_sub = (window * data)[::sub_step]\n",
    "\n",
    "tapered_subsampled = pd.DataFrame(tapered_sub)\n",
    "tapered_subsampled['date_column'] = df1.index[::sub_step] # Add date column\n",
    "tapered_subsampled.rename(columns={0: 'Subsampled_Tapered'}, inplace=True)\n",
    "tapered_subsampled.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3eab49d6-eb4d-4462-8298-a6f23667546b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FFT plot display\n",
    "# (plotly wasn't working here)\n",
    "def apply_hann_window(data):\n",
    "    \"\"\"Apply Hann window and return windowed data with coherent gain factor.\"\"\"\n",
    "    window = np.hanning(len(data))\n",
    "    windowed_data = data * window\n",
    "    # Coherent gain for Hann window is 0.5, so need to multiply by 2 later on to make correction\n",
    "    coherent_gain = 0.5\n",
    "    return windowed_data, coherent_gain\n",
    "\n",
    "\n",
    "def subsample_with_antialiasing(data, decimation_factor, order, sr=1):\n",
    "    \"\"\"Subsample data using scipy.signal.decimate with anti-aliasing filter.\n",
    "    data: array\n",
    "    decimation_factor: factor by which to reduce sampling rate\n",
    "    sr: Original sampling rate (1 sample/day)\n",
    "    order: order of the filter\n",
    "    \"\"\"\n",
    "    # scipy.signal.decimate applies an anti-aliasing filter automatically\n",
    "    # Use a higher order filter for better anti-aliasing (default is 8)\n",
    "    decimated_data = scipy.signal.decimate(data, decimation_factor, order, ftype='iir', zero_phase=True)\n",
    "    # get new sampling rate:\n",
    "    new_sr = sr / decimation_factor\n",
    "    return decimated_data, new_sr\n",
    "\n",
    "\n",
    "def plotly_fft_2(data, sr, fig1=None, fig2=None, fig3=None, name='', apply_window=True):\n",
    "    \"\"\"Plot FFT with windowing correction.\n",
    "    data: array\n",
    "    sr: Sampling rate\n",
    "    apply_window : bool, says whether to apply Hann window (default True)\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Apply window if requested\n",
    "    if apply_window:\n",
    "        windowed_data, coherent_gain = apply_hann_window(data)\n",
    "    else:\n",
    "        windowed_data = data\n",
    "        coherent_gain = 1.0\n",
    "    \n",
    "    # Compute FFT\n",
    "    fft_values = scipy.fft.rfft(windowed_data)\n",
    "    freqs = scipy.fft.rfftfreq(n, d=1/sr)  # d = 1/sr because freqs should be in cycles/day\n",
    "    freqs_per_year = freqs * 365.25\n",
    "    \n",
    "    # Apply coherent gain correction to magnitudes\n",
    "    magnitudes = np.abs(fft_values) * (2.0/n) / coherent_gain\n",
    "    \n",
    "    # DC component and Nyquist (if present) should not be doubled\n",
    "    magnitudes[0] /= 2.0\n",
    "    if n % 2 == 0:  # If even length, Nyquist frequency exists\n",
    "        magnitudes[-1] /= 2.0\n",
    "    \n",
    "    phases = np.angle(fft_values)\n",
    "    \n",
    "    # Create figures:\n",
    "    if fig1 is None:\n",
    "        fig1 = plt.figure(figsize=(10, 6))\n",
    "        plt.plot(freqs_per_year, magnitudes, linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (Linear)', fontsize=16)\n",
    "        plt.xlabel('Frequency (1/yr)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        plt.minorticks_on()\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    else:\n",
    "        plt.figure(fig1.number)\n",
    "        plt.plot(freqs_per_year, magnitudes, linewidth=1, label=name)\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    if fig2 is None:\n",
    "        fig2 = plt.figure(figsize=(10, 6))\n",
    "        plt.loglog(freqs_per_year[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (log)', fontsize=16)\n",
    "        plt.xlabel('Frequency (1/yr)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        #plt.axvline(x=3.5, color='green', label='3.5 cycles/yr', linestyle='--')\n",
    "        #plt.axvline(x=182.625, color='red', label='182.625 cycles/yr', linestyle='--')\n",
    "        #plt.axvline(x=1.826, color='green', label='1.826 cycles/yr', linestyle='--')\n",
    "        #plt.axvline(x=0.5479, color='red', label='0.5479 cycles/yr', linestyle='--')\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    else:\n",
    "        plt.figure(fig2.number)\n",
    "        plt.loglog(freqs_per_year[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.legend()\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    if fig3 is None:\n",
    "        fig3 = plt.figure(figsize=(10, 6))\n",
    "        plt.plot(freqs_per_year[1:], phases[1:], linewidth=1, label=name)\n",
    "        plt.title('Phase Spectrum', fontsize=16)\n",
    "        plt.xlabel('Frequency (1/yr)', fontsize=14)\n",
    "        plt.ylabel('Phase (radians)', fontsize=14)\n",
    "        plt.grid(True, alpha=0.5)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        #plt.xlim(-0.1, 5)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='lower right', framealpha=0.5)\n",
    "    else:\n",
    "        plt.figure(fig3.number)\n",
    "        plt.plot(freqs_per_year[1:], phases[1:], linewidth=1, label=name)\n",
    "        if name:\n",
    "            plt.legend(loc='lower right', framealpha=0.8)\n",
    "    \n",
    "    return fig1, fig2, fig3\n",
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your dataframe df with column 'Normalized_Shifted'\n",
    "\n",
    "decimation_factor = 52  # step for subsampling\n",
    "sr = 1  # Original sampling rate: 1 sample/day\n",
    "\n",
    "# For full data\n",
    "fig1, fig2, fig3 = plotly_fft_2(df1['Normalized_Shifted'].values, sr, name='Full Data', apply_window=False)\n",
    "\n",
    "# For subsampled data with anti-aliasing\n",
    "tapered_sub1, new_sr = subsample_with_antialiasing(df1['Normalized_Shifted'].values, decimation_factor, 8, sr)\n",
    "plotly_fft_2(tapered_sub1, new_sr, fig1=fig1, fig2=fig2, fig3=fig3, name='Subsampled/Tapered Data', apply_window=True)\n",
    "\n",
    "# Print sampling information\n",
    "print(f\"Original sampling rate: {sr} sample/day\")\n",
    "print(f\"Original Nyquist: {sr/2} cycle/day\")\n",
    "print(f\"Original Nyquist in cycles/year {sr/2 * 365.25} cycle/year\")\n",
    "print(f\"Decimation factor: {decimation_factor}\")\n",
    "new_sr_example = sr / decimation_factor\n",
    "print(f\"New sampling rate: {new_sr_example} sample/day\")\n",
    "print(f\"New Nyquist: {new_sr_example/2} cycle/day\")\n",
    "print(f\"New Nyquist in cycles/year: {new_sr_example/2 * 365.25} cycle/year\")\n",
    "\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb744fc5-a54d-4edb-9a03-58eb275d858b",
   "metadata": {},
   "source": [
    "***\n",
    "### Adding Sine Waves to Full DNS Data\n",
    "#### Notes:\n",
    "- two data columns containing sine waves are added to the data frame containing the full DNS data (not tapered or subsampled yet), along with additional columns for the combined sine waves, the time index, and the DNS data combined with both sine waves\n",
    "- the first year of DNS data is also plotted alongside the individual sine waves before combining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25f111ec-a9ef-4335-9d54-60414f015cc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = df1['Normalized_Shifted'].to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40bfdae-2e61-45c0-87e6-f03f52feb06d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add sine waves to df2 data columns\n",
    "def add_sine_waves(data, data_to_add_sines_to=''):\n",
    "    data = data.copy()\n",
    "    \n",
    "    data['time_column'] = np.arange(len(data))\n",
    "    t = data['time_column'].values # time is in days and so is the data collection, so index matches day value\n",
    "    \n",
    "    # Define sine wave parameters\n",
    "    frequencies = {'sine_0_05_cpd': 0.005,  # cpd: cycles per day\n",
    "                'sine_0_15_cpd': 0.0015}\n",
    "    \n",
    "    amplitudes = {'sine_0_05_cpd': 0.25,\n",
    "                'sine_0_15_cpd': 0.25}\n",
    "    \n",
    "    phases = {'sine_0_05_cpd': 0,\n",
    "           'sine_0_15_cpd': 0}\n",
    "    \n",
    "    # Add sine wave columns\n",
    "    for wave_name, freq in frequencies.items():\n",
    "        amplitude = amplitudes[wave_name]\n",
    "        phase = phases[wave_name]\n",
    "        \n",
    "        # Calculate sine wave: A * sin(2π * f * t + φ)\n",
    "        data[wave_name] = amplitude * np.sin(2 * np.pi * freq * t + phase)\n",
    "        \n",
    "    # Add combined sine waves column\n",
    "    sine_columns = list(frequencies.keys())\n",
    "    data['combined_sines'] = data[sine_columns].sum(axis=1)\n",
    "\n",
    "    # Add sine waves to original data        \n",
    "    if data_to_add_sines_to in data.columns:\n",
    "        data['Data_With_Sines'] = data[data_to_add_sines_to] + data['combined_sines']\n",
    "\n",
    "    if 'time_column' in data.index:\n",
    "        data = data.drop('time_column', axis=0) # prevents time_column from appearing as an additional row\n",
    "\n",
    "    return data\n",
    "\n",
    "add_sine_waves(df2, data_to_add_sines_to='Normalized_Shifted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3baf75a1-e470-41d7-b084-2d59ff461ab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sine_waves(df2)\n",
    "print(df2.columns.tolist())  # Check the exact column names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "588dbb11-f99f-4e38-ad28-2b970cef295a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot time series with separate sine waves:\n",
    "df2_with_sines = add_sine_waves(df2, data_to_add_sines_to='Normalized_Shifted') # need new variable for edited dataframe\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(df2_with_sines['time_column'][:365], df2_with_sines['Normalized_Shifted'][:365], label='Normalized time series')\n",
    "plt.plot(df2_with_sines['time_column'][:365], df2_with_sines['sine_0_05_cpd'][:365], label='sine_0_005_cpd')\n",
    "plt.plot(df2_with_sines['time_column'][:365], df2_with_sines['sine_0_15_cpd'][:365], label='sine_0_0015_cpd', linestyle=\":\")\n",
    "plt.xlabel(\"Time (days)\")\n",
    "plt.ylabel(\"Amplitude\")\n",
    "plt.title(\"Sine Waves Before Adding to Data (First Year)\")\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.tight_layout()\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Trying with plotly:\n",
    "pre_sines = go.Figure()\n",
    "pre_sines.add_trace(go.Scatter(x= list(df1.index), y= list(df2_with_sines['Normalized_Shifted']), mode='lines', name='Time Series'))\n",
    "pre_sines.add_trace(go.Scatter(x= list(df1.index), y= list(df2_with_sines['sine_0_05_cpd']), mode='lines', name='0.005 cpd sine'))\n",
    "pre_sines.add_trace(go.Scatter(x= list(df1.index), y= list(df2_with_sines['sine_0_15_cpd']), mode='lines', name='0.0015 cpd sine'))\n",
    "# Set title\n",
    "pre_sines.update_layout(title_text=\"Before Adding Sine Waves to Water Level Data (1960-2024)\", width=1100, height=600, \n",
    "                  xaxis_title='Date', yaxis_title='Water Level', title_x=0.5, showlegend=True)\n",
    "# Add range slider\n",
    "pre_sines.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "pre_sines.show()\n",
    "# Zoom in to see more than blob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22185c5b-293a-4474-9254-f766b3f3c1d2",
   "metadata": {},
   "source": [
    "***\n",
    "### Adding Sine Waves to Subsampled/Tapered Time Series\n",
    "#### Notes:\n",
    "- using add_sine_waves function from previous cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a52b33c1-2fdb-4473-b3f2-c81f07c5ee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_sine_waves(tapered_subsampled, data_to_add_sines_to='Subsampled_Tapered')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24196cdd-a117-428a-9d5f-bd879d699721",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_t_with_sines = add_sine_waves(tapered_subsampled, data_to_add_sines_to='Subsampled_Tapered')\n",
    "\n",
    "# Plotting time series with sines before combining:\n",
    "pre_2sines = go.Figure()\n",
    "pre_2sines.add_trace(go.Scatter(x= list(tapered_subsampled['date_column']), y= list(sub_t_with_sines['Subsampled_Tapered']), mode='lines', \n",
    "                                name='Tapered Time Series (step = 52)', zorder=10))\n",
    "pre_2sines.add_trace(go.Scatter(x= list(tapered_subsampled['date_column']), y= list(sub_t_with_sines['sine_0_05_cpd']), mode='lines', \n",
    "                                name='0.005 cpd sine', line=dict(dash='solid')))\n",
    "pre_2sines.add_trace(go.Scatter(x= list(tapered_subsampled['date_column']), y= list(sub_t_with_sines['sine_0_15_cpd']), mode='lines', \n",
    "                                name='0.0015 cpd sine', line=dict(dash='solid')))\n",
    "# Set title\n",
    "pre_2sines.update_layout(title_text=\"Subsampled and Tapered Water Level Data Before Adding Sine Waves (1960-2024)\", width=1100, height=600, \n",
    "                  xaxis_title='Date', yaxis_title='Water Level', title_x=0.5, showlegend=True)\n",
    "# Add range slider\n",
    "pre_2sines.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "pre_2sines.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fac73143-ff9e-4190-8168-cc62b730aca5",
   "metadata": {},
   "source": [
    "***\n",
    "### Fast Fourier Transform for Subsampled and Tapered DNS Data with Added Sine Waves\n",
    "#### Notes:\n",
    "- Calling plotly_fft2 function from a previous cell\n",
    "- Sine spikes should appear at 1.826 cycles/yr and 0.5479 cycles/year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a1910a-d9fd-4086-943c-08b497aa203e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to fix display of above sine FFT plots same way as above\n",
    "decimation_factor = 52  # step for subsampling\n",
    "sr = 1  # Original sampling rate: 1 sample/day\n",
    "\n",
    "# For full data\n",
    "fig1, fig2, fig3 = plotly_fft_2(df2_with_sines['Data_With_Sines'].values, sr, name='Full Data', apply_window=False)\n",
    "\n",
    "# For subsampled data with anti-aliasing\n",
    "tapered_sub1, new_sr = subsample_with_antialiasing(df2_with_sines['Data_With_Sines'].values, decimation_factor, 8, sr) \n",
    "plotly_fft_2(tapered_sub1, new_sr, fig1=fig1, fig2=fig2, fig3=fig3, name='Subsampled/Tapered Data', apply_window=True)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4498a0b-507b-4555-87b0-4d5e371618f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "freq1 = 0.0015  # cycles/day\n",
    "freq2 = 0.005   # cycles/day\n",
    "\n",
    "period1 = 1/freq1  # days\n",
    "period2 = 1/freq2  # days\n",
    "\n",
    "samples_per_cycle1 = period1 / 52\n",
    "samples_per_cycle2 = period2 / 52\n",
    "\n",
    "print(f\"Sine wave 1: {samples_per_cycle1:.2f} samples per cycle\")\n",
    "print(f\"Sine wave 2: {samples_per_cycle2:.2f} samples per cycle\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba38cf4d-1236-40e8-b06a-42afa7f052ec",
   "metadata": {},
   "source": [
    "***\n",
    "### Continuous Wavelet Transform (Full DNS Data)\n",
    "#### Notes:\n",
    "- the Morlet wavelet, given by $\\psi(t)=\\exp(\\frac{-t^2}{2})\\cos(5t)$, is used for the wavelet transform\n",
    "- scalograms are created using the full DNS data and the full DNS data combined with sine waves\n",
    "- histograms of scalogram magnitudes are also plotted showing the percentages corresponding to certain magnitude ranges in addition to other statistics (i.e. mean, median, mode, etc.)\n",
    "- a difference map was also created to compare scalograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93dd531c-1d3e-4691-80d9-6aa813c10d19",
   "metadata": {},
   "source": [
    "***\n",
    "### Increasing Scalogram Frequency Resolution\n",
    "#### Notes:\n",
    "- **goal**: increase frequency resolution of scalograms\n",
    "  \n",
    "    **Consequences of increasing frequency resolution:**\n",
    "    - increasing the frequency resolution will decrease the time resolution\n",
    "    - will change appearance of added sine waves\n",
    "      \n",
    "    <br> **Possible Methods for increasing frequency resolution:**\n",
    "    - Increasing the number of scales will provide more frequency bins to sample, which will show finer differences between frequencies\n",
    "    - Adjust spacing to linear instead of logarithmic and focus on a specific frequency range of interest\n",
    "    - Decrease time resolution\n",
    "      <br>**Effective Methods:**\n",
    "      - Increasing the number of scales showed a slight difference compared to the starting scalogram\n",
    "      - Decreasing time resolution by applying a uniform filter from scipy.ndimage had the greatest effect on the appearance of the plots\n",
    "          - The filter is set up to only smooth in time and leave frequency unchanged\n",
    "<br>\n",
    "\n",
    "- Scales have been determined based on the formula $f=\\frac{F_c}{a\\cdot\\Delta t}$, where $f$ is the pseudo-frequency corresponding to the scale, $F_c$ is the approx. center frequency of the wavelet, $a$ is the wavelet scale, and $\\Delta t$ is the sampling period of the signal\n",
    "- Additionally, scales are chosen so that the frequency range of the plots do not exceed the Nyquist frequency"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9043bf5d-b853-41a6-a898-272c83105a37",
   "metadata": {},
   "source": [
    "#### Full DNS Data Without Added Sine Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1871468d-c5c5-4cd0-821f-247c7da01342",
   "metadata": {},
   "outputs": [],
   "source": [
    "# continuous wavelet transform without sine wave added to data\n",
    "# Convert scale to frequency:\n",
    "data = df2['Normalized_Shifted'].values \n",
    "data = data.flatten() \n",
    "#scales = np.logspace(np.log10(1), np.log10(100), 120)\n",
    "\n",
    "t = df2_with_sines['time_column'].values\n",
    "dt = t[1] - t[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "#scale_min = 1.0 / (10 * dt)  # Small scale for high freq, use freq_max=10\n",
    "freq_max_years = 150 # below Nyquist limit of ~182.6\n",
    "scale_min = 1.0/(freq_max_years/ 365.25)\n",
    "scale_max = 1.0 / (0.001 * dt)  # Large scale for low freq, use freq_min=0.001\n",
    "scales = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "coefficients, frequencies = pywt.cwt(data, scales, 'morl')\n",
    "s2f_per_year = frequencies*365.25\n",
    "\n",
    "# Smooth only in the TIME direction (axis=1), not frequency (axis=0)\n",
    "smoothed_coefficients = uniform_filter(np.abs(coefficients), size=(1, 20))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# try log scale:\n",
    "plt.pcolormesh(t, s2f_per_year, np.log10(smoothed_coefficients), cmap='plasma', shading='auto', vmin=-5)\n",
    "plt.colorbar(label='log\\u2081\\u2080(Magnitude)')\n",
    "plt.ylabel('Frequency (1/Year)')\n",
    "plt.yscale('log')\n",
    "# Convert time to years\n",
    "years = np.arange(1960, 2025, 10)\n",
    "year_days = (years - 1960) * 365.25\n",
    "plt.xticks(year_days, years)\n",
    "plt.xlabel('Year')\n",
    "plt.title('Toronto Daily Mean Water Level Without Added Sine Waves (1960-2024)') # from normalized + detrended + mean-shifted water level data\n",
    "plt.show()\n",
    "print(f\"Frequency range: {s2f_per_year.min():.3f} to {s2f_per_year.max():.3f} cycles/year\")\n",
    "\n",
    "# Add histogram:\n",
    "magnitudes = np.abs(smoothed_coefficients)\n",
    "magnitude_values = magnitudes.flatten()\n",
    "# Then create histogram\n",
    "fig_hist = go.Figure()\n",
    "\n",
    "fig_hist.add_trace(go.Histogram(x=magnitude_values, nbinsx=50,  # Number of bins\n",
    "        histnorm='percent',  # Options: '', 'percent', 'probability', 'density', 'probability density'\n",
    "        name='Scalogram Magnitudes', marker=dict(opacity=0.7, line=dict(color='darkblue', width=1))))\n",
    "\n",
    "# Add mean magnitude line:\n",
    "mean_mags = np.mean(magnitude_values)\n",
    "fig_hist.add_vline( x=mean_mags, line_dash=\"dash\", line_color=\"red\")\n",
    "# Add median magnitude line:\n",
    "median_mags = np.median(magnitude_values)\n",
    "fig_hist.add_vline(x=median_mags, line_dash=\"dash\", line_color=\"orange\")\n",
    "# Get mode:\n",
    "# Find bin with maximum count\n",
    "hist_counts, bin_edges = np.histogram(magnitude_values, bins=50)\n",
    "mode_bin_index = np.argmax(hist_counts)\n",
    "# Mode is the center of the bin with the highest count:\n",
    "mode_value = (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1]) / 2\n",
    "fig_hist.add_vline(x=mode_value, line_dash=\"dash\", line_color=\"limegreen\")\n",
    "\n",
    "# Add traces for legend entries\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='red', dash='dash'),name=f'Mean: {mean_mags:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='orange', dash='dash'),name=f'Median: {median_mags:.2f}',showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='limegreen', dash='dash'),name=f'Mode: {mode_value:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.update_layout(title='Histogram of Scalogram Magnitudes', xaxis=dict(title='Magnitude'), yaxis=dict(title='Percentage'),\n",
    "        template='plotly_white', width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Log scale for large span of magnitudes\n",
    "fig_hist.update_yaxes(type=\"log\")\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "print(ss.describe(magnitude_values))\n",
    "std_dev = np.std(magnitude_values)\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b9dc2f-6e21-4971-9d04-66f481a0a079",
   "metadata": {},
   "source": [
    "#### Full DNS Data With Added Sine Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7238547e-0314-4a9d-ab9b-e2587237ff21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continuous wavelet transform with sine waves added\n",
    "data1 = df2_with_sines['Data_With_Sines'].values  # Normalized data, Convert to numpy array\n",
    "data1 = data1.flatten()  # Need data to be 1D\n",
    "print(f\"Data shape: {data1.shape}\") # Check data shape\n",
    "t = df2_with_sines['time_column'].values\n",
    "dt = t[1] - t[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "#scale_min = 1.0 / (10 * dt)  # Small scale for high freq, use freq_max=10\n",
    "freq_max_years = 150 # below Nyquist limit of ~182.6\n",
    "scale_min = 1.0/(freq_max_years/ 365.25)\n",
    "scale_max = 1.0 / (0.001 * dt)  # Large scale for low freq, use freq_min=0.001\n",
    "scales1 = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "# plot with years instead of days\n",
    "t_years = t / 365.25  # Convert days to years\n",
    "# CWT:\n",
    "coefficients1, frequencies1 = pywt.cwt(data1, scales1, 'morl', sampling_period=1.0)\n",
    "print(f\"Coefficients shape: {coefficients1.shape}\")\n",
    "# Convert frequencies from cycles/day to cycles/year\n",
    "s2f_per_year1 = frequencies1*365.25\n",
    "\n",
    "# Trying to apply uniform_filter from scipy.ndimage to decrease time resolution and make frequency content easier to see\n",
    "# Smooth only in the TIME direction (axis=1), not frequency (axis=0)\n",
    "coefficients_smoothed = uniform_filter(np.abs(coefficients1), size=(1, 20))\n",
    "# size=(rows, columns) or size=(frequency_axis, time_axis)\n",
    "# for size=(1, 10): No smoothing in frequency, smooth over 10 time points in time\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "plt.pcolormesh(t_years, s2f_per_year1, (coefficients_smoothed), cmap='plasma', shading='auto', vmax=8) \n",
    "plt.colorbar(label='Magnitude')\n",
    "#plt.colorbar(label='log\\u2081\\u2080(Magnitude)')\n",
    "plt.ylabel('Frequency (1/year)')\n",
    "plt.yscale('log')\n",
    "plt.xlabel('Time (years)')\n",
    "plt.title('Toronto Daily Mean Water Level With Added Sine Waves (1960-2024)') # from normalized + detrended + mean-shifted water level data\n",
    "plt.show()\n",
    "\n",
    "# Add histogram:\n",
    "magnitudes = np.abs(coefficients_smoothed)\n",
    "magnitude_values = magnitudes.flatten()\n",
    "# Then create histogram\n",
    "fig_hist = go.Figure()\n",
    "\n",
    "fig_hist.add_trace(go.Histogram(x=magnitude_values, nbinsx=50,  # Number of bins\n",
    "        histnorm='percent',  # Options: '', 'percent', 'probability', 'density', 'probability density'\n",
    "        name='Scalogram Magnitudes', marker=dict(opacity=0.7, line=dict(color='darkblue', width=1))))\n",
    "\n",
    "# Add mean magnitude line:\n",
    "mean_mags = np.mean(magnitude_values)\n",
    "fig_hist.add_vline( x=mean_mags, line_dash=\"dash\", line_color=\"red\")\n",
    "# Add median magnitude line:\n",
    "median_mags = np.median(magnitude_values)\n",
    "fig_hist.add_vline(x=median_mags, line_dash=\"dash\", line_color=\"orange\")\n",
    "# Get mode:\n",
    "# Find bin with maximum count\n",
    "hist_counts, bin_edges = np.histogram(magnitude_values, bins=50)\n",
    "mode_bin_index = np.argmax(hist_counts)\n",
    "# Mode is the center of the bin with the highest count:\n",
    "mode_value = (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1]) / 2\n",
    "fig_hist.add_vline(x=mode_value, line_dash=\"dash\", line_color=\"limegreen\")\n",
    "\n",
    "# Add traces for legend entries\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='red', dash='dash'),name=f'Mean: {mean_mags:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='orange', dash='dash'),name=f'Median: {median_mags:.2f}',showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='limegreen', dash='dash'),name=f'Mode: {mode_value:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.update_layout(title='Histogram of Scalogram Magnitudes', xaxis=dict(title='Magnitude'), yaxis=dict(title='Percentage'),\n",
    "        template='plotly_white', width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Log scale for large span of magnitudes\n",
    "fig_hist.update_yaxes(type=\"log\")\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "print(ss.describe(magnitude_values))\n",
    "std_dev = np.std(magnitude_values)\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fefc3f9d-3535-4b32-b921-a2596f3ef5c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Difference map:\n",
    "# trying to make difference map for smoothed subsampled/tapered data\n",
    "signal_original = df2['Normalized_Shifted'].values.flatten() # WITHOUT sine waves\n",
    "\n",
    "signal_with_sine = df2_with_sines['Data_With_Sines'].values.flatten()  # WITH sine waves\n",
    "\n",
    "# Get the time indices from subsampled data\n",
    "t = df2_with_sines['time_column'].values\n",
    "# Account for the step size of 52 \n",
    "t_years = 1960 + (t / 365.25)  # Convert days to years)\n",
    "dt = t[1] - t[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "#scale_min = 1.0 / (10 * dt)  # Small scale for high freq, use freq_max=10\n",
    "freq_max_years = 150 # below Nyquist limit of ~182.6\n",
    "scale_min = 1.0/(freq_max_years/ 365.25)\n",
    "scale_max = 1.0 / (0.001 * dt)  # Large scale for low freq, use freq_min=0.001\n",
    "scales1 = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "\n",
    "coefficients_original, freqs = pywt.cwt(signal_original, scales, 'morl')\n",
    "coefficients_with_sine, freqs = pywt.cwt(signal_with_sine, scales, 'morl')\n",
    "s2f_per_year = freqs*365.25\n",
    "\n",
    "coefficients_smoothed = uniform_filter(np.abs(coefficients_original), size=(1, 20))\n",
    "sines_coefficients_smoothed = uniform_filter(np.abs(coefficients_with_sine), size=(1,20))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# Scalogram 1 - use log scale like original\n",
    "im1 = axes[0].pcolormesh(t_years, s2f_per_year, np.log10(coefficients_smoothed), cmap='plasma', shading='auto', vmin=-5)\n",
    "axes[0].set_title('Scalogram 1')\n",
    "axes[0].set_xlabel('Year')  # or 'Time (years)'\n",
    "axes[0].set_ylabel('Frequency (1/year)')\n",
    "axes[0].set_yscale('log')  # Use log scale for frequency axis\n",
    "plt.colorbar(im1, ax=axes[0], label='log\\u2081\\u2080(Magnitude)')\n",
    "\n",
    "# Scalogram 2\n",
    "im2 = axes[1].pcolormesh(t_years, s2f_per_year, (sines_coefficients_smoothed), cmap='plasma', shading='auto', vmax=8) \n",
    "axes[1].set_title('Scalogram 2')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Frequency (1/year)')\n",
    "axes[1].set_yscale('log')\n",
    "plt.colorbar(im2, ax=axes[1], label='Magnitude')\n",
    "\n",
    "# Normalize each scalogram\n",
    "coeff_norm = (np.abs(coefficients_smoothed) - np.abs(coefficients_smoothed).min()) / (np.abs(coefficients_smoothed).max() - np.abs(coefficients_smoothed).min())\n",
    "coeff1_norm = (np.abs(sines_coefficients_smoothed) - np.abs(sines_coefficients_smoothed).min()) / (np.abs(sines_coefficients_smoothed).max() - np.abs(sines_coefficients_smoothed).min())\n",
    "\n",
    "difference = coeff_norm - coeff1_norm\n",
    "# Difference map\n",
    "max_diff = np.max(np.abs(difference))\n",
    "im3 = axes[2].pcolormesh(t_years, s2f_per_year, difference, cmap='RdBu', vmin=-max_diff, vmax=max_diff, shading='auto')\n",
    "axes[2].set_title('Difference (1 - 2)')\n",
    "axes[2].set_xlabel('Year')\n",
    "axes[2].set_ylabel('Frequency (1/year)')\n",
    "axes[2].set_yscale('log')\n",
    "plt.colorbar(im3, ax=axes[2], label='Difference')\n",
    "\n",
    "print(\"Comparing scalograms for full DNS data with and without added sine waves:\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e8f5ca-3fab-4671-9580-0f5a3341ac89",
   "metadata": {},
   "source": [
    "***\n",
    "### Continuous Wavelet Transform (Subsampled/Tapered DNS Data)\n",
    "#### Notes:\n",
    "- The Morlet wavelet, given by $\\psi(t)=\\exp(\\frac{-t^2}{2})\\cos(5t)$, is used for the wavelet transform\n",
    "- Again, histograms and difference maps were created for the scalograms"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0987685-d4e2-46c4-9590-1e4926c3a7d4",
   "metadata": {},
   "source": [
    "#### Subsampled and Tapered DNS Data Without Added Sine Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71357df2-3aa3-479a-820f-fef9a9530dcd",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr=1\n",
    "def apply_hann_window(data):\n",
    "    # Want to apply Hann window and return windowed data with coherent gain factor\n",
    "    window = np.hanning(len(data))\n",
    "    windowed_data = data * window\n",
    "    # Coherent gain for Hann window is 0.5, so need to multiply by 2 to make correction\n",
    "    coherent_gain = 0.5\n",
    "    return windowed_data/coherent_gain\n",
    "\n",
    "def subsample_with_antialiasing(data, decimation_factor, sr):\n",
    "    \"\"\"Subsample data using scipy.signal.decimate with anti-aliasing filter.\n",
    "    data: array\n",
    "    decimation_factor: factor by which to reduce sampling rate\n",
    "    sr: Original sampling rate\"\"\"\n",
    "    \n",
    "    # scipy.signal.decimate applies an anti-aliasing filter automatically\n",
    "    # Use a higher order filter for better anti-aliasing (default is 8)\n",
    "    decimated_data = scipy.signal.decimate(data, decimation_factor, ftype='iir', zero_phase=True)\n",
    "    # get new sampling rate:\n",
    "    new_sr = sr / decimation_factor\n",
    "    return decimated_data\n",
    "\n",
    "hann_windowed_data = apply_hann_window(df2['Normalized_Shifted'].values)\n",
    "hann_windowed_sines = apply_hann_window(df2_with_sines['Data_With_Sines'].values)\n",
    "subsample_with_antialiasing(hann_windowed_data, 52, sr)\n",
    "subsample_with_antialiasing(hann_windowed_sines, 52, sr)\n",
    "# doesn't make noticable difference for scalograms below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "697f0157-be63-4de4-b8f8-baaf574d2dcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## CHECK (make sure sub/tap scalograms are showing up correctly)\n",
    "sub_t_data = sub_t_with_sines['Subsampled_Tapered'].values.flatten()\n",
    "\n",
    "# Get the time indices from subsampled data\n",
    "t = sub_t_with_sines['time_column'].values\n",
    "# Account for the step size of 52 -> IMPORTANT\n",
    "sampling_interval_days = 52  # subsampled every 52nd day\n",
    "t_days = t * sampling_interval_days  # Convert indices to actual days\n",
    "t_years = 1960 + (t_days / 365.25)  # Convert days to years\n",
    "dt = t_days[1] - t_days[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "\n",
    "scale_min = 1.0 / (0.01 * dt)  # Small scale for high freq, use freq_max=0.01\n",
    "scale_max = 1.0 / (0.00001 * dt)  # Large scale for low freq, use freq_min=0.00001\n",
    "# note: trying to use same scales for full and subsampled/tapered data is not working\n",
    "scales = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "\n",
    "coefficients, frequencies = pywt.cwt(sub_t_data, scales, 'morl')\n",
    "s2f_per_year = frequencies*365.25\n",
    "\n",
    "# smooth in time\n",
    "subt_coefficients_smoothed = uniform_filter(np.abs(coefficients), size=(1, 5))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# try log scale:\n",
    "plt.pcolormesh(t_years, s2f_per_year, np.log10(subt_coefficients_smoothed), cmap='plasma', shading='auto', vmin=-5)\n",
    "plt.colorbar(label='log\\u2081\\u2080(Magnitude)')\n",
    "plt.ylabel('Frequency (1/Year)')\n",
    "plt.yscale('log')\n",
    "# Convert time to years\n",
    "years = np.arange(1960, 2025, 10)\n",
    "plt.xticks(years)\n",
    "plt.xlabel('Year')\n",
    "plt.title('Subsampled and Tapered Toronto Daily Mean Water Level Data Without Added Sine Waves (1960-2024)')\n",
    "plt.show()\n",
    "print(f\"Frequency range: {s2f_per_year.min():.3f} to {s2f_per_year.max():.3f} cycles/year\")\n",
    "\n",
    "# Add histogram:\n",
    "magnitudes = np.abs(subt_coefficients_smoothed)\n",
    "magnitude_values = magnitudes.flatten()\n",
    "# Then create histogram\n",
    "fig_hist = go.Figure()\n",
    "\n",
    "fig_hist.add_trace(go.Histogram(x=magnitude_values, nbinsx=50,  # Number of bins\n",
    "        histnorm='percent',  # Options: '', 'percent', 'probability', 'density', 'probability density'\n",
    "        name='Scalogram Magnitudes', marker=dict(opacity=0.7, line=dict(color='darkblue', width=1))))\n",
    "\n",
    "# Add mean magnitude line:\n",
    "mean_mags = np.mean(magnitude_values)\n",
    "fig_hist.add_vline( x=mean_mags, line_dash=\"dash\", line_color=\"red\")\n",
    "# Add median magnitude line:\n",
    "median_mags = np.median(magnitude_values)\n",
    "fig_hist.add_vline(x=median_mags, line_dash=\"dash\", line_color=\"orange\")\n",
    "# Get mode:\n",
    "# Find bin with maximum count\n",
    "hist_counts, bin_edges = np.histogram(magnitude_values, bins=50)\n",
    "mode_bin_index = np.argmax(hist_counts)\n",
    "# Mode is the center of the bin with the highest count:\n",
    "mode_value = (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1]) / 2\n",
    "fig_hist.add_vline(x=mode_value, line_dash=\"dash\", line_color=\"limegreen\")\n",
    "\n",
    "# Add traces for legend entries\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='red', dash='dash'),name=f'Mean: {mean_mags:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='orange', dash='dash'),name=f'Median: {median_mags:.2f}',showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='limegreen', dash='dash'),name=f'Mode: {mode_value:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.update_layout(title='Histogram of Scalogram Magnitudes', xaxis=dict(title='Magnitude'), yaxis=dict(title='Percentage'),\n",
    "        template='plotly_white', width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Log scale for large span of magnitudes\n",
    "fig_hist.update_yaxes(type=\"log\")\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "print(ss.describe(magnitude_values))\n",
    "std_dev = np.std(magnitude_values)\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87b7052b-3d91-449b-b10a-f3d7908d44a1",
   "metadata": {},
   "source": [
    "#### Subsampled and Tapered Data With Added Sine Waves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a78179-0364-4ca3-8717-5324335bd6e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "sub_t_sines_data = sub_t_with_sines['Data_With_Sines'].values.flatten() \n",
    "\n",
    "# Get the time indices from subsampled data\n",
    "t = sub_t_with_sines['time_column'].values\n",
    "# Account for the step size of 52 \n",
    "sampling_interval_days = 52  # subsampled every 52nd day\n",
    "t_days = t * sampling_interval_days  # Convert indices to actual days\n",
    "t_years = 1960 + (t_days / 365.25)  # Convert days to years)\n",
    "dt = t_days[1] - t_days[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "\n",
    "scale_min = 1.0 / (0.01 * dt)  # Small scale for high freq, use freq_max=0.01\n",
    "scale_max = 1.0 / (0.00001 * dt)  # Large scale for low freq, use freq_min=0.00001\n",
    "scales = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "\n",
    "coefficients, frequencies = pywt.cwt(sub_t_sines_data, scales, 'morl')\n",
    "s2f_per_year = frequencies*365.25\n",
    "\n",
    "# smooth in time\n",
    "tsub_coefficients_smoothed = uniform_filter(np.abs(coefficients), size=(1, 10))\n",
    "\n",
    "plt.figure(figsize=(14, 6))\n",
    "# try log scale:\n",
    "#plt.pcolormesh(t_years, s2f_per_year, np.log10(tsub_coefficients_smoothed), cmap='plasma', shading='auto', vmin=-5)\n",
    "#plt.colorbar(label='log\\u2081\\u2080(Magnitude)')\n",
    "plt.pcolormesh(t_years, s2f_per_year, tsub_coefficients_smoothed, cmap='plasma', shading='auto', vmin=-0.2)\n",
    "plt.colorbar(label='Magnitude')\n",
    "plt.ylabel('Frequency (1/Year)')\n",
    "plt.yscale('log')\n",
    "#plt.axhline(y=0.5479, color='white', linestyle='--', label='Sine 2')\n",
    "#plt.axhline(y=1.826, color='skyblue', linestyle='--', label='Sine 1')\n",
    "# Convert time to years\n",
    "years = np.arange(1960, 2025, 10)\n",
    "plt.xticks(years)\n",
    "plt.xlabel('Year')\n",
    "plt.title('Subsampled and Tapered Toronto Daily Mean Water Level Data With Added Sine Waves (1960-2024)')\n",
    "plt.show()\n",
    "print(f\"Frequency range: {s2f_per_year.min():.3f} to {s2f_per_year.max():.3f} cycles/year\")\n",
    "\n",
    "# Add histogram:\n",
    "magnitudes = np.abs(tsub_coefficients_smoothed)\n",
    "magnitude_values = magnitudes.flatten()\n",
    "# Then create histogram\n",
    "fig_hist = go.Figure()\n",
    "\n",
    "fig_hist.add_trace(go.Histogram(x=magnitude_values, nbinsx=50,  # Number of bins\n",
    "        histnorm='percent',  # Options: '', 'percent', 'probability', 'density', 'probability density'\n",
    "        name='Scalogram Magnitudes', marker=dict(opacity=0.7, line=dict(color='darkblue', width=1))))\n",
    "\n",
    "# Add mean magnitude line:\n",
    "mean_mags = np.mean(magnitude_values)\n",
    "fig_hist.add_vline( x=mean_mags, line_dash=\"dash\", line_color=\"red\")\n",
    "# Add median magnitude line:\n",
    "median_mags = np.median(magnitude_values)\n",
    "fig_hist.add_vline(x=median_mags, line_dash=\"dash\", line_color=\"orange\")\n",
    "# Get mode:\n",
    "# Find bin with maximum count\n",
    "hist_counts, bin_edges = np.histogram(magnitude_values, bins=50)\n",
    "mode_bin_index = np.argmax(hist_counts)\n",
    "# Mode is the center of the bin with the highest count:\n",
    "mode_value = (bin_edges[mode_bin_index] + bin_edges[mode_bin_index + 1]) / 2\n",
    "fig_hist.add_vline(x=mode_value, line_dash=\"dash\", line_color=\"limegreen\")\n",
    "\n",
    "# Add traces for legend entries\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines', line=dict(color='red', dash='dash'),name=f'Mean: {mean_mags:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='orange', dash='dash'),name=f'Median: {median_mags:.2f}',showlegend=True))\n",
    "\n",
    "fig_hist.add_trace(go.Scatter(x=[None], y=[None], mode='lines',line=dict(color='limegreen', dash='dash'),name=f'Mode: {mode_value:.2f}', showlegend=True))\n",
    "\n",
    "fig_hist.update_layout(title='Histogram of Scalogram Magnitudes', xaxis=dict(title='Magnitude'), yaxis=dict(title='Percentage'),\n",
    "        template='plotly_white', width=1000, height=600, showlegend=True)\n",
    "\n",
    "# Log scale for large span of magnitudes\n",
    "fig_hist.update_yaxes(type=\"log\")\n",
    "\n",
    "fig_hist.show()\n",
    "\n",
    "print(ss.describe(magnitude_values))\n",
    "std_dev = np.std(magnitude_values)\n",
    "print(f\"Standard Deviation: {std_dev:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be91faef-9ab4-4be4-94a0-02028a468453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# trying to make difference map for smoothed subsampled/tapered data\n",
    "st_signal_original = sub_t_with_sines['Subsampled_Tapered'].values.flatten() # WITHOUT sine waves\n",
    "\n",
    "st_signal_with_sine = sub_t_with_sines['Data_With_Sines'].values.flatten()  # WITH sine waves\n",
    "\n",
    "# Get the time indices from subsampled data\n",
    "t = sub_t_with_sines['time_column'].values\n",
    "# Account for the step size of 52 \n",
    "sampling_interval_days = 52  # subsampled every 52nd day\n",
    "t_days = t * sampling_interval_days  # Convert indices to actual days\n",
    "t_years = 1960 + (t_days / 365.25)  # Convert days to years)\n",
    "dt = t_days[1] - t_days[0]\n",
    "print(f\"dt: {dt} day\")\n",
    "\n",
    "scale_min = 1.0 / (0.01 * dt)  # Small scale for high freq, use freq_max=0.01\n",
    "scale_max = 1.0 / (0.00001 * dt)  # Large scale for low freq, use freq_min=0.00001\n",
    "scales = np.logspace(np.log10(scale_min), np.log10(scale_max), 300)\n",
    "\n",
    "coefficients_original_st, freqs = pywt.cwt(st_signal_original, scales, 'morl')\n",
    "coefficients_with_sine_st, freqs = pywt.cwt(st_signal_with_sine, scales, 'morl')\n",
    "s2f_per_year = freqs*365.25\n",
    "\n",
    "tsub_coefficients_smoothed = uniform_filter(np.abs(coefficients_original_st), size=(1, 10))\n",
    "tsub_sines_coefficients_smoothed = uniform_filter(np.abs(coefficients_with_sine_st), size=(1,10))\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "# Scalogram 1 - use log scale like original\n",
    "im1 = axes[0].pcolormesh(t_years, s2f_per_year, np.log10(tsub_coefficients_smoothed), cmap='plasma', shading='auto', vmin=-5)\n",
    "axes[0].set_title('Scalogram 1')\n",
    "axes[0].set_xlabel('Year')  # or 'Time (years)'\n",
    "axes[0].set_ylabel('Frequency (1/year)')\n",
    "axes[0].set_yscale('log')  # Use log scale for frequency axis\n",
    "plt.colorbar(im1, ax=axes[0], label='log\\u2081\\u2080(Magnitude)')\n",
    "\n",
    "# Scalogram 2\n",
    "im2 = axes[1].pcolormesh(t_years, s2f_per_year, tsub_sines_coefficients_smoothed, cmap='plasma', shading='auto')\n",
    "axes[1].set_title('Scalogram 2')\n",
    "axes[1].set_xlabel('Year')\n",
    "axes[1].set_ylabel('Frequency (1/year)')\n",
    "axes[1].set_yscale('log')\n",
    "plt.colorbar(im2, ax=axes[1], label='Magnitude')\n",
    "\n",
    "# Normalize each scalogram\n",
    "coeff_norm = (np.abs(tsub_coefficients_smoothed) - np.abs(tsub_coefficients_smoothed).min()) / (np.abs(tsub_coefficients_smoothed).max() - np.abs(tsub_coefficients_smoothed).min())\n",
    "coeff1_norm = (np.abs(tsub_sines_coefficients_smoothed) - np.abs(tsub_sines_coefficients_smoothed).min()) / (np.abs(tsub_sines_coefficients_smoothed).max() - np.abs(tsub_sines_coefficients_smoothed).min())\n",
    "\n",
    "difference = coeff_norm - coeff1_norm\n",
    "# Difference map\n",
    "max_diff = np.max(np.abs(difference))\n",
    "im3 = axes[2].pcolormesh(t_years, s2f_per_year, difference, cmap='RdBu', vmin=-max_diff, vmax=max_diff, shading='auto')\n",
    "axes[2].set_title('Difference (1 - 2)')\n",
    "axes[2].set_xlabel('Year')\n",
    "axes[2].set_ylabel('Frequency (1/year)')\n",
    "axes[2].set_yscale('log')\n",
    "plt.colorbar(im3, ax=axes[2], label='Difference')\n",
    "\n",
    "print(\"Comparing scalograms for subsampled/tapered data with and without added sine waves:\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63afcc50-4fac-45fc-a257-55549356a4ae",
   "metadata": {},
   "source": [
    "#### Possible issues with scalograms:\n",
    "- Horizontal banding near bottom of plots, in particular in the subsampled/tapered scalogram without sine waves added"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
