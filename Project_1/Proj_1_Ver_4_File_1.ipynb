{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3cc6c96-9f74-4e77-af16-8fe76e0d26c8",
   "metadata": {},
   "source": [
    "# Water Level Data\n",
    "\n",
    "### Code Summary:\n",
    "\n",
    "- Detrending, normalizing, and shifting data to zero mean\n",
    "- Subsampled data plots\n",
    "- Tapered data plots\n",
    "- Interactive plotly plots (Note: plotly plots work best for smaller code file sizes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d0f78a-9094-4476-97e7-dd62ff27fffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.fft\n",
    "import pywt\n",
    "import plotly.graph_objects as go\n",
    "import plotly.tools as tls\n",
    "from plotly.subplots import make_subplots\n",
    "from scipy.signal.windows import hann\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import uniform_filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d29f65-8229-4cde-b3c5-5c13f2aecfe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = pd.read_csv(\"Toronto-daily-mean-water-1960-2024.csv\", index_col=\"Obs_date\")\n",
    "\n",
    "# Check for null values in dataframe:\n",
    "any_null = df1.isnull().any().any()\n",
    "print(any_null) # Will show false if no null values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e01674ed-8520-422e-bb80-f6b11e64f191",
   "metadata": {},
   "source": [
    "***\n",
    "### Plotting the Time Series\n",
    "#### Notes:\n",
    "- Plot of the original water level data with no changes made\n",
    "- Use the horizontal slider to zoom in/out of certain date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8398bad-032f-4545-a2dd-3021fd3f30ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Interactive plot of time series \n",
    "fig = go.Figure()\n",
    "fig.add_trace(go.Scatter(x= list(df1.index), y= list(df1['SLEV(metres)']), mode='lines', name='Time Series'))\n",
    "# Set title\n",
    "fig.update_layout(title_text=\"Toronto Daily Mean Water Level (1960-2024)\", width=1100, height=600, \n",
    "                  xaxis_title='Date', yaxis_title='Water Level (m)', title_x=0.5)\n",
    "# Add range slider\n",
    "fig.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "fig.show()\n",
    "\n",
    "print(ss.describe(df1['SLEV(metres)']))\n",
    "print(\"mode:\", ss.mode(df1['SLEV(metres)'])) \n",
    "print(\"standard deviation:\", df1['SLEV(metres)'].std())\n",
    "print(\"median:\", df1[\"SLEV(metres)\"].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eed57c-6170-43d9-b332-422ffcd49ac4",
   "metadata": {},
   "source": [
    "***\n",
    "### Normalizing, Detrending and Shifting Data\n",
    "#### Notes:\n",
    "- data range of time series was normalized using MinMaxScaler from sklearn\n",
    "- data was linearly detrended using signal.detrend from scipy\n",
    "- the mean of the data was shifted to approximately zero, as shown by the red dashed line on the plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50a9d41c-dfd9-431f-a98b-4e3552701a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearly detrend data, normalize and shift to zero mean:\n",
    "df1['Detrended'] = signal.detrend(df1['SLEV(metres)'], type='linear')\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "df1['Normalized'] = scaler.fit_transform(df1['Detrended'].values.reshape(-1, 1)).flatten()\n",
    "print(f\"Min-Max normalized range: [{df1['Normalized'].min():.3f}, {df1['Normalized'].max():.3f}]\")\n",
    "\n",
    "# Shift Mean:\n",
    "df1['Normalized_Shifted'] = (df1['Normalized'] - 0.5)\n",
    "print(f\"New range: [{df1['Normalized_Shifted'].min():.3f}, {df1['Normalized_Shifted'].max():.3f}]\")\n",
    "print(f\"New mean: {df1['Normalized_Shifted'].mean():.6f}\")\n",
    "\n",
    "# Plot:\n",
    "dshifted_norm_plot = go.Figure()\n",
    "dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index), y= list(df1['Normalized_Shifted']), mode='lines', name='Time Series'))\n",
    "dshifted_norm_plot.update_layout(title_text=\"Toronto Daily Mean Water Level (1960-2024) <br> (Detrended, Normalized, Shifted)\",\n",
    "                                 width=1100, height=600, xaxis_title='Date', yaxis_title='Normalized Water Level', title_x=0.5)\n",
    "dshifted_norm_plot.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "\n",
    "# Calculate the overall mean:\n",
    "overall_mean = np.round(df1['Normalized_Shifted'].mean())\n",
    "# Add a horizontal line for the overall mean\n",
    "dshifted_norm_plot.add_shape(type=\"line\", x0=df1.index.min(), y0=overall_mean, x1=df1.index.max(), y1=overall_mean, \n",
    "                    line=dict(color=\"Red\", width=2, dash=\"dash\"), name=f\"Overall Mean ({overall_mean})\", showlegend=True)\n",
    "\n",
    "dshifted_norm_plot.show()\n",
    "\n",
    "# Stats for plot\n",
    "print(ss.describe(df1['Normalized_Shifted']))\n",
    "print(\"median:\", df1[\"Normalized_Shifted\"].median())\n",
    "print(\"mode:\", ss.mode(df1[\"Normalized_Shifted\"])) \n",
    "print(\"standard deviation:\", df1[\"Normalized_Shifted\"].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bece2a2c-de2a-4d3d-b200-c5873569d174",
   "metadata": {},
   "source": [
    "***\n",
    "### Subsampling the Time Series\n",
    "#### Notes:\n",
    "- different step sizes were applied to the normalized, detrended and mean-shifted (DNS) data for subsampling\n",
    "- click on the legend titles to select or hide certain time series\n",
    "- use the horizontal slider to zoom in/out of certain date ranges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73d74674-9115-4824-a6ae-6597a3a0e88b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subsampling time series:\n",
    "# Number of data points in df1['Normalized_Shifted']: 23292\n",
    "step1 = len(df1['Normalized_Shifted'])//100\n",
    "step2 = len(df1['Normalized_Shifted'])//1000\n",
    "step3 = len(df1['Normalized_Shifted'])//10000\n",
    "step4 = 1000\n",
    "step5 = 100\n",
    "step6 = 10\n",
    "\n",
    "# Plot:\n",
    "dshifted_norm_plot = go.Figure()\n",
    "dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index), y= list(df1['Normalized_Shifted']), mode='lines', name='Time Series (no step)'))\n",
    "#dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::step1], y= list(df1['Normalized_Shifted'][::step1]), mode='lines', \n",
    "#                                        name=f'Time Series (step={step1})'))\n",
    "#dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::step2], y= list(df1['Normalized_Shifted'][::step2]), mode='lines', \n",
    "#                                        name=f'Time Series (step={step2})'))\n",
    "#dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::step3], y= list(df1['Normalized_Shifted'][::step3]), mode='lines', \n",
    "#                                        name=f'Time Series (step={step3})'))\n",
    "#dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::step6], y= list(df1['Normalized_Shifted'][::step6]), mode='lines', \n",
    "#                                        name=f'Time Series (step={step6})'))\n",
    "dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::52], y= list(df1['Normalized_Shifted'][::52]), mode='lines', \n",
    "                                        name=f'Time Series (step={52})'))\n",
    "#dshifted_norm_plot.add_trace(go.Scatter(x= list(df1.index)[::step5], y= list(df1['Normalized_Shifted'][::step5]), mode='lines', \n",
    "#                                        name=f'Time Series (step={step5})'))\n",
    "dshifted_norm_plot.update_layout(title_text=\"Toronto Daily Mean Water Level (1960-2024) <br> Subsampling\",\n",
    "                                 width=1100, height=600, xaxis_title='Date', yaxis_title='Normalized Water Level', title_x=0.5)\n",
    "dshifted_norm_plot.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "dshifted_norm_plot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cff1f86-158d-48f0-a6dc-06c42ea36497",
   "metadata": {},
   "outputs": [],
   "source": [
    "# stats for subsample step 52:\n",
    "print(\"data step: 52\")\n",
    "print(ss.describe(df1['Normalized_Shifted'][::52]))\n",
    "print(\"mode:\", ss.mode(df1['Normalized_Shifted'][::52])) \n",
    "print(\"standard deviation:\", df1['Normalized_Shifted'][::52].std())\n",
    "print(\"median:\", df1[\"Normalized_Shifted\"][::52].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ab03294-71fd-4b25-b36f-657bdf776535",
   "metadata": {},
   "source": [
    "### Comparing Subsampled Time Series to Full Time Series\n",
    "#### Notes:\n",
    "- The subsampled time series of different step sizes were compared to the full detrended, normalized and shifted (DNS) time series\n",
    "- A variety of comparison methods were tested that could be applied elsewhere as well (e.g. for comparing tapered vs untapered data):\n",
    "    - **Comparing summary statistics** (i.e. mean, median, standard deviation) for different sampling rates against the full DNS series\n",
    "    - Calculating the **mean absolute error (MAE)** between the full DNS data and reconstructions of the subsampled data\n",
    "        - interpolation was used to reconstruct the time series after subsampling to align timestamps between the subsampled data and the original DNS data\n",
    "        - the MAE was then calculated and expressed as a percentage of the DNS series' standard deviation\n",
    "    - Defining a **'percent_check' function** to determine if the subsampled data is within 5% of the full DNS data\n",
    "        - this method also uses interpolation to compare subsampled data to the full DNS data\n",
    "        - the absolute error between the full DNS data and the reconstructed subsampled data is calculated and divided by the full data range\n",
    "        - this result is then used determine if the error is within the selected threshold (i.e. 5%)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "893c4108-c70d-4c32-a8a5-d142fd2433a6",
   "metadata": {},
   "source": [
    "#### Comparing Summary Statistics for Different Subsampling Intervals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd58a48d-6e6d-4928-af65-007eac1817d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare summary statistics for different steps\n",
    "stats_original = {'Mean': df1['Normalized_Shifted'].iloc[::].mean(),\n",
    "    'Std': df1['Normalized_Shifted'].iloc[::].std(),\n",
    "    'Median': df1['Normalized_Shifted'].iloc[::].median()}\n",
    "\n",
    "stats_step1 = {'Mean': df1['Normalized_Shifted'].iloc[::step1].mean(),\n",
    "    'Std': df1['Normalized_Shifted'].iloc[::step1].std(),\n",
    "    'Median': df1['Normalized_Shifted'].iloc[::step1].median()}\n",
    "\n",
    "stats_step2 = {'Mean': df1['Normalized_Shifted'].iloc[::step2].mean(),\n",
    "    'Std': df1['Normalized_Shifted'].iloc[::step2].std(),\n",
    "    'Median': df1['Normalized_Shifted'].iloc[::step2].median()}\n",
    "\n",
    "stats_step3 = {'Mean': df1['Normalized_Shifted'].iloc[::step3].mean(),\n",
    "    'Std': df1['Normalized_Shifted'].iloc[::step3].std(),\n",
    "    'Median': df1['Normalized_Shifted'].iloc[::step3].median()}\n",
    "\n",
    "# Percent differences in statistics\n",
    "for stat in ['Mean', 'Std', 'Median']:\n",
    "    pct_o1 = abs(stats_original[stat] - stats_step1[stat]) / abs(stats_original[stat]) * 100\n",
    "    pct_o2 = abs(stats_original[stat] - stats_step2[stat]) / abs(stats_original[stat]) * 100\n",
    "    pct_o3 = abs(stats_original[stat] - stats_step3[stat]) / abs(stats_original[stat]) * 100\n",
    "    pct_12 = abs(stats_step1[stat] - stats_step2[stat]) / abs(stats_step1[stat]) * 100\n",
    "    pct_13 = abs(stats_step1[stat] - stats_step3[stat]) / abs(stats_step1[stat]) * 100\n",
    "    print(f\"{stat} % diff (original vs step1): {pct_o1:.2f}%\")\n",
    "    print(f\"{stat} % diff (original vs step2): {pct_o2:.2f}%\")\n",
    "    print(f\"{stat} % diff (original vs step3): {pct_o3:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25194c54-ea44-41cf-887c-fa18b1005698",
   "metadata": {},
   "source": [
    "#### Comparing Subsampled DNS Data to Full DNS Data Using Mean Absolute Error (MAE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2832b924-1779-4474-a501-20efc2c04125",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare each subsampled version against the original time series\n",
    "# trying to use Mean Absolute Error (MAE)\n",
    "# use interpolation to align timestamps\n",
    "# Original time series:\n",
    "x_original = np.arange(len(df1))\n",
    "y_original = df1['Normalized_Shifted'].values\n",
    "\n",
    "# Create interpolation functions for each subsampled version\n",
    "x_step1 = np.arange(0, len(df1), step1)\n",
    "y_step1 = df1['Normalized_Shifted'].iloc[::step1].values\n",
    "f_step1 = interpolate.interp1d(x_step1, y_step1, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "x_step2 = np.arange(0, len(df1), step2)\n",
    "y_step2 = df1['Normalized_Shifted'].iloc[::step2].values\n",
    "f_step2 = interpolate.interp1d(x_step2, y_step2, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "x_step3 = np.arange(0, len(df1), step3)\n",
    "y_step3 = df1['Normalized_Shifted'].iloc[::step3].values\n",
    "f_step3 = interpolate.interp1d(x_step3, y_step3, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "x_step5 = np.arange(0, len(df1), step5)\n",
    "y_step5 = df1['Normalized_Shifted'].iloc[::step5].values\n",
    "f_step5 = interpolate.interp1d(x_step5, y_step5, kind='linear', fill_value='extrapolate')\n",
    "\n",
    "# Reconstruct time series from subsampled data:\n",
    "y_reconstructed_step1 = f_step1(x_original)\n",
    "y_reconstructed_step2 = f_step2(x_original)\n",
    "y_reconstructed_step3 = f_step3(x_original)\n",
    "y_reconstructed_step5 = f_step5(x_original)\n",
    "\n",
    "# Trying mean absolute error (MAE), then can also express as a percentage of the data's standard deviation\n",
    "mae_1S = np.mean(np.abs(y_original - y_reconstructed_step1))\n",
    "mae_2S = np.mean(np.abs(y_original - y_reconstructed_step2))\n",
    "mae_3S = np.mean(np.abs(y_original - y_reconstructed_step3))\n",
    "\n",
    "# Express as percentage of original data's standard deviation\n",
    "original_std = np.std(y_original)\n",
    "print(f\"Standard Deviation of Full Data: {original_std: .4f}\")\n",
    "percent_diff_1S = (mae_1S / original_std)*100\n",
    "percent_diff_2S = (mae_2S / original_std)*100\n",
    "percent_diff_3S = (mae_3S / original_std)*100\n",
    "\n",
    "print(f\"Step 1 Subsample: Step = {step1}, Mean abs error: {mae_1S:.4f}, {percent_diff_1S:.2f}% of the original data's standard deviation\")\n",
    "print(f\"Step 2 Subsample: Step = {step2}, Mean abs error: {mae_2S:.4f}, {percent_diff_2S:.2f}% of the original data's standard deviation\")\n",
    "print(f\"Step 3 Subsample: Step = {step3}, Mean abs error: {mae_3S:.4f}, {percent_diff_3S:.2f}% of the original data's standard deviation\")\n",
    "# low % is good, indicates small reconstruction error compared to signal itself"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cedc2508-c1bf-4af2-888b-d9eda42b9968",
   "metadata": {},
   "source": [
    "#### Comparing Subsampled DNS Data to Full DNS Data Using 'percent_check' Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35254a9-551b-4d06-bb3b-aed3e5780677",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Idea: define function that checks if subsampled data is within 5% of starting data\n",
    "def percent_check(data, column, step, tolerance=0.05):\n",
    "    # Get original series\n",
    "    x_original = np.arange(len(data))\n",
    "    y_original = data[column].values\n",
    "    \n",
    "    # Get subsampled series\n",
    "    x_subsampled = np.arange(0, len(data), step)\n",
    "    y_subsampled = data[column].iloc[::step].values\n",
    "    \n",
    "    # Interpolate subsampled data back to original timestamps\n",
    "    f_interp = interpolate.interp1d(x_subsampled, y_subsampled, kind='linear', fill_value='extrapolate')\n",
    "    y_reconstructed = f_interp(x_original)\n",
    "    \n",
    "    # Calculate absolute errors\n",
    "    abs_errors = np.abs(y_original - y_reconstructed)\n",
    "    \n",
    "    # Calculate data range (the scale of the data)\n",
    "    data_range = np.max(y_original) - np.min(y_original)\n",
    "    # Normalize errors by data range instead of individual values to get scale-independent percentages\n",
    "    normalized_errors = abs_errors / data_range\n",
    "\n",
    "    # After calculating normalized_errors\n",
    "    # plt.figure(figsize=(12, 4))\n",
    "    # plt.plot(normalized_errors*100, alpha=0.5)\n",
    "    # plt.axhline(tolerance*100, color='r', linestyle='--', label=f'{tolerance*100}% tolerance')\n",
    "    # plt.ylabel('Normalized Error (%)')\n",
    "    # plt.xlabel('Data Point')\n",
    "    # plt.title(f'Reconstruction Error (Step={step})')\n",
    "    # plt.legend()\n",
    "    # plt.show()\n",
    "    \n",
    "    # Test if within tolerance\n",
    "    within_tolerance = normalized_errors <= tolerance # check each point\n",
    "    pct_within = (np.sum(within_tolerance) / len(within_tolerance))*100 # then count how many pass\n",
    "    \n",
    "    # Overall pass/fail\n",
    "    passes = pct_within >= 95  # At least 95% of points within tolerance\n",
    "    \n",
    "    results = {\n",
    "        'step': step,\n",
    "        'tolerance': f'{tolerance*100}%',\n",
    "        'passes': passes,\n",
    "        'points_within_tolerance': f'{pct_within: .4f}%',\n",
    "        'num_original_points': len(y_original),\n",
    "        'num_subsampled_points': len(y_subsampled),\n",
    "        'data_range': f'{data_range: .4f}',  \n",
    "        'max_normalized_error': f'{(np.max(normalized_errors)*100): .4f}%' }\n",
    "    \n",
    "    return results\n",
    "\n",
    "#results_step1 = percent_check(df1, 'Normalized_Shifted', step1, tolerance=0.05)\n",
    "#results_step2 = percent_check(df1, 'Normalized_Shifted', step2, tolerance=0.05)\n",
    "results_step3 = percent_check(df1, 'Normalized_Shifted', step3, tolerance=0.05)\n",
    "#print(\"results1:\", results_step1)\n",
    "#print(\"results2:\", results_step2)\n",
    "print(\"results3:\", results_step3)\n",
    "#results_step4 = percent_check(df1, 'Normalized_Shifted', step4, tolerance=0.05)\n",
    "#results_step5 = percent_check(df1, 'Normalized_Shifted', step5, tolerance=0.05)\n",
    "#results_step6 = percent_check(df1, 'Normalized_Shifted', step6, tolerance=0.05)\n",
    "#print(\"results4:\", results_step4)\n",
    "#print(\"results5:\", results_step5)\n",
    "#print(\"results6:\", results_step6)\n",
    "results_step_test = percent_check(df1, 'Normalized_Shifted', 53, tolerance=0.05)\n",
    "print(\"results for step test\", results_step_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ea9063-ba43-444e-b39e-79d2ff1f8dc7",
   "metadata": {},
   "source": [
    "***\n",
    "### Tapering the Time Series\n",
    "#### Notes:\n",
    "- a Hann window/raised cosine was used to taper the DNS time series\n",
    "- the time series was padded with zeros to create different tapers:\n",
    "    - no padding\n",
    "    - padding with half the length of the original time series\n",
    "    - padding with the full length of the original time series\n",
    "    - etc.\n",
    "- click on legend titles in the plot to select or hide certain series\n",
    "- use the horizontal slider to zoom in/out of certain date ranges\n",
    "- the tapered series with no padding does not have discontinuities at the edges of the series, however, the other series do\n",
    "- the same methods of comparison that were used to compare subsampled to full data were used to compare the tapered data to the original DNS series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ee7981c-5905-434d-ba29-faf5ddca5c57",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tapering the time series\n",
    "L = len(df1['Normalized_Shifted'])\n",
    "data = df1['Normalized_Shifted'].values\n",
    "time_index = pd.to_datetime(df1.index)\n",
    "# time delta (1 day for water level data)\n",
    "time_delta = (time_index[1] - time_index[0])\n",
    "\n",
    "# 1L - original taper\n",
    "window1 = hann(L)\n",
    "tapered_data1 = window1 * data\n",
    "padding1 = 0\n",
    "\n",
    "# 2L - gentler taper\n",
    "padding2 = L // 2\n",
    "data2_padded = np.concatenate([np.zeros(padding2), data, np.zeros(padding2)])\n",
    "window2 = hann(len(data2_padded))\n",
    "tapered_data2_full = window2 * data2_padded\n",
    "time_index2 = pd.date_range(start=time_index[0] - (time_delta * padding2), periods=len(data2_padded), freq=time_delta)\n",
    "\n",
    "# 3L - very gentle taper (3L = full length of padding-data-padding)\n",
    "padding3 = L\n",
    "data3_padded = np.concatenate([np.zeros(padding3), data, np.zeros(padding3)])\n",
    "window3 = hann(len(data3_padded))\n",
    "tapered_data3_full = window3 * data3_padded\n",
    "time_index3 = pd.date_range(start=time_index[0] - (time_delta * padding3), periods=len(data3_padded), freq=time_delta)\n",
    "\n",
    "# test taper -> 1.5+1+1.5= 4  (4l = full length of padding-data-padding)\n",
    "padding4 = (3*L)//2\n",
    "data4_padded = np.concatenate([np.zeros(padding4), data, np.zeros(padding4)])\n",
    "window4 = hann(len(data4_padded))\n",
    "tapered_data4_full = window4*data4_padded\n",
    "time_index4 = pd.date_range(start=time_index[0] - (time_delta*padding4), periods=len(data4_padded), freq=time_delta)\n",
    "\n",
    "# Plot\n",
    "tapered_plot = go.Figure()\n",
    "tapered_plot.add_trace(go.Scatter(x=list(time_index), y=list(data), mode='lines', name='Original 1L Time Series'))\n",
    "tapered_plot.add_trace(go.Scatter(x=list(time_index), y=list(tapered_data1), mode='lines', name='1L Tapered Series'))\n",
    "tapered_plot.add_trace(go.Scatter(x=list(time_index2), y=list(tapered_data2_full), mode='lines', name='2L Tapered Series'))\n",
    "tapered_plot.add_trace(go.Scatter(x=list(time_index3), y=list(tapered_data3_full), mode='lines', name='3L Tapered Series'))\n",
    "tapered_plot.add_trace(go.Scatter(x=list(time_index4), y=list(tapered_data4_full), mode='lines', name='4L Tapered Series'))\n",
    "\n",
    "tapered_plot.update_layout(title_text=\"Toronto Daily Mean Water Level (1960-2024) <br> Different Taper Lengths\", width=1100, height=600,\n",
    "    xaxis_title='Date', yaxis_title='Normalized Water Level', title_x=0.5)\n",
    "tapered_plot.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "tapered_plot.show()\n",
    "\n",
    "# Need percent difference between tapered and untapered plots\n",
    "# Trying mean absolute error (MAE), then can also express as a percentage of the data's standard deviation\n",
    "# Calculate MAE for each taper:\n",
    "mae_1L = np.mean(np.abs(data - tapered_data1))\n",
    "mae_2L = np.mean(np.abs(data - tapered_data2_full[padding2:-padding2]))\n",
    "mae_3L = np.mean(np.abs(data - tapered_data3_full[padding3:-padding3]))\n",
    "\n",
    "# Express as percentage of original data's standard deviation\n",
    "std_original = np.std(data)\n",
    "percent_diff_1L = (mae_1L / std_original)*100\n",
    "percent_diff_2L = (mae_2L / std_original)*100\n",
    "percent_diff_3L = (mae_3L / std_original)*100\n",
    "\n",
    "print(f\"1L Taper: Mean abs error: {mae_1L:.4f}, {percent_diff_1L:.2f}% of the original data's standard deviation\")\n",
    "print(f\"2L Taper: Mean abs error: {mae_2L:.4f}, {percent_diff_2L:.2f}% of the original data's standard deviation\")\n",
    "print(f\"3L Taper: Mean abs error: {mae_3L:.4f}, {percent_diff_3L:.2f}% of the original data's standard deviation\")\n",
    "\n",
    "# stats for tapered series:\n",
    "print(\"\")\n",
    "print(\"Summary Stats:\")\n",
    "print(\"1L Taper\")\n",
    "print(ss.describe(tapered_data1))\n",
    "print(\"mode:\", ss.mode(tapered_data1)) \n",
    "print(\"standard deviation:\", tapered_data1.std())\n",
    "print(\"median:\", np.median(tapered_data1))\n",
    "print(\"\")\n",
    "print(\"2L Taper\")\n",
    "print(ss.describe(tapered_data2_full))\n",
    "print(\"mode:\", ss.mode(tapered_data2_full)) \n",
    "print(\"standard deviation:\", tapered_data2_full.std())\n",
    "print(\"median:\", np.median(tapered_data2_full))\n",
    "print(\"\")\n",
    "print(\"3L Taper\")\n",
    "print(ss.describe(tapered_data3_full))\n",
    "print(\"mode:\", ss.mode(tapered_data3_full)) \n",
    "print(\"standard deviation:\", tapered_data3_full.std())\n",
    "print(\"median:\", np.median(tapered_data3_full))\n",
    "print(\"\")\n",
    "print(\"4L Taper\")\n",
    "print(ss.describe(tapered_data4_full))\n",
    "print(\"mode:\", ss.mode(tapered_data4_full)) \n",
    "print(\"standard deviation:\", tapered_data4_full.std())\n",
    "print(\"median:\", np.median(tapered_data4_full))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "091b631b-5ca2-481d-bd5d-a2c3ffba2036",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare tapered time series to original \n",
    "def percent_check_tapered(original_data, tapered_data_full, padding, tolerance=0.05):\n",
    "    \"\"\"original_data : Original unpadded time series (array)\n",
    "    tapered_data_full : Tapered time series, includes padding (array)\n",
    "    padding : Amount of padding added to each end (int)\n",
    "    tolerance : Acceptable error as fraction of data range (float)\"\"\"\n",
    "    # Extract original from tapered data (remove padding)\n",
    "    if padding > 0:\n",
    "        tapered_original_region = tapered_data_full[padding:-padding]\n",
    "    else:\n",
    "        tapered_original_region = tapered_data_full\n",
    "    \n",
    "    # Direct comparison: tapered vs original\n",
    "    abs_errors = np.abs(original_data - tapered_original_region)\n",
    "    # normalization\n",
    "    data_range = np.max(original_data) - np.min(original_data)\n",
    "    normalized_errors = abs_errors / data_range\n",
    "    \n",
    "    # Calculate metrics\n",
    "    within_tolerance = normalized_errors <= tolerance\n",
    "    pct_within = (np.sum(within_tolerance) / len(within_tolerance))*100\n",
    "    \n",
    "    # MAE metrics\n",
    "    mae = np.mean(abs_errors)\n",
    "    std_original = np.std(original_data)\n",
    "    mae_pct_std = (mae / std_original)*100\n",
    "    \n",
    "    passes = pct_within >= 95\n",
    "    \n",
    "    results = {\n",
    "        'padding': padding,\n",
    "        'tolerance': f'{tolerance*100}%',\n",
    "        'passes': passes,\n",
    "        'points_within_tolerance': f'{pct_within: .4f}%',\n",
    "        'max_normalized_error': f'{(np.max(normalized_errors)*100): .4f}%',\n",
    "        'mean_abs_error': f'{mae: .4f}',\n",
    "        'mae_as_%_of_std': f'{mae_pct_std: .4f}%',\n",
    "        'num_points': len(original_data),\n",
    "        'data_range': f'{data_range: .4f}'}\n",
    "    \n",
    "    return results\n",
    "\n",
    "results_1 = percent_check_tapered(data, tapered_data1, padding1, tolerance=0.05)\n",
    "results_2 = percent_check_tapered(data, tapered_data2_full, padding2, tolerance=0.05)\n",
    "results_3 = percent_check_tapered(data, tapered_data3_full, padding3, tolerance=0.05)\n",
    "results_4 = percent_check_tapered(data, tapered_data4_full, padding4, tolerance=0.05)\n",
    "print(\"results1:\", results_1)\n",
    "print(\"\")\n",
    "print(\"results2:\", results_2)\n",
    "print(\"\")\n",
    "print(\"results3:\", results_3)\n",
    "print(\"\")\n",
    "print(\"results4:\", results_4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267169db-5fdc-4d96-ac8c-3738086c04ea",
   "metadata": {},
   "source": [
    "***\n",
    "### Combining Subsampling and Tapering\n",
    "#### Notes:\n",
    "- Tapering with Hann window and using step=52 based on results from percent_check function for subsampled data\n",
    "- taper applied with no padding to prevent discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47db3c6-907f-4ca7-bff7-315ac139faed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From previous subsampling and tapering related code, determined that step = 52 goes well with percent_check,\n",
    "# and taper with no padding prevents discontinuities\n",
    "sub_step = 52\n",
    "\n",
    "L = len(df1['Normalized_Shifted'])\n",
    "data = df1['Normalized_Shifted'].values\n",
    "time_index = pd.to_datetime(df1.index)\n",
    "# time delta (1 day for water level data)\n",
    "time_delta = (time_index[1] - time_index[0])\n",
    "\n",
    "window = hann(L)\n",
    "tapered_sub = (window * data)[::sub_step]\n",
    "\n",
    "# Plot:\n",
    "sub_t = go.Figure()\n",
    "sub_t.add_trace(go.Scatter(x= list(time_index), y= list(df1['Normalized_Shifted']), mode='lines', name='Full Time Series')) \n",
    "sub_t.add_trace(go.Scatter(x= list(time_index)[::sub_step], y= list(tapered_sub), mode='lines', name=f'Tapered Time Series (step={sub_step})'))\n",
    "sub_t.update_layout(title_text=\"Tapered and Subsampled Toronto Daily Mean Water Level (1960-2024)\", width=1100, height=600,\n",
    "    xaxis_title='Date', yaxis_title='Normalized Water Level', title_x=0.5)\n",
    "sub_t.update_layout(xaxis=dict(rangeslider=dict(visible=True)))\n",
    "sub_t.show()\n",
    "\n",
    "# Summary Stats for tapered/subsampled series:\n",
    "print(\"Summary Stats:\")\n",
    "print(ss.describe(tapered_sub))\n",
    "print(\"mode:\", ss.mode(tapered_sub)) \n",
    "print(\"standard deviation:\", tapered_sub.std())\n",
    "print(\"median:\", np.median(tapered_sub))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0a40dc7-0a5b-4593-8839-0487eead39fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# see next file for adding sine waves, FFT plots and scalograms"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
