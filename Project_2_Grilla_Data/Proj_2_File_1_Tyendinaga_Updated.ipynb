{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1ab54a30-cdcf-435f-9c91-6c3488f82d5e",
   "metadata": {},
   "source": [
    "### Tyendinaga Data\n",
    "##### Notes:\n",
    "- data exported from 'Student' computer\n",
    "- using data with filename Tyendinaga1_001.asc as main data source to make plots with\n",
    "- Tyendinaga1_001.asc data was recorded on Oct. 26, 2025 from 18:07:12 to 18:27:12"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f53c5960-c7b8-4338-ba99-e7f5eaa20084",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from scipy import signal\n",
    "import scipy.stats as ss\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "import scipy.fft\n",
    "from scipy.signal.windows import hann\n",
    "from scipy import interpolate\n",
    "from scipy.ndimage import uniform_filter\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from scipy.signal import savgol_filter\n",
    "from scipy.signal import hilbert\n",
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fedea2a-a0a7-4d8b-95c6-c0f6caf8d1b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of the library versions used:\n",
    "print(\"Version Notes:\")\n",
    "print(\"pandas:\", pd.__version__)\n",
    "#print(\"matplotlib.pyplot:\", plt.__version__) # no version\n",
    "print(\"numpy:\", np.__version__)\n",
    "print(\"scipy:\", scipy.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15da4c34-e3fa-4255-94d9-7741d34221ef",
   "metadata": {},
   "source": [
    "##### DataFrame --> File Name Legend:\n",
    "- df --> Tyendinaga_001.asc\n",
    "- df2 --> Tyendinaga2_001.asc\n",
    "- df3 --> Tyendinaga3_001.asc\n",
    "- df4 --> Tyendinaga4_001.asc\n",
    "- df5 --> Tyendinaga5_001.asc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc908e75-6f77-4263-bf0c-49c927bdd6d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: Tyendinaga1\n",
    "column_names = ['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ']\n",
    "df = pd.read_table('Tyendinaga1_001.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=32, names=column_names)\n",
    "#df.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "548547e0-eaea-4453-a806-e32edd8d9a6b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: Tyendinaga2\n",
    "# Loading the rest of the Tyendinaga files so can make comparitive plots between datasets\n",
    "column_names2 = ['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ']\n",
    "df2 = pd.read_table('Tyendinaga2_001.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=32, names=column_names2)\n",
    "#df2.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdbf829-470b-4efe-8d4c-04aaf00d2b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: Tyendinaga3\n",
    "column_names3 = ['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ']\n",
    "df3 = pd.read_table('Tyendinaga3_001.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=32, names=column_names3)\n",
    "#df3.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d49b472d-7257-47a3-8c00-986e7faa6b85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: Tyendinaga4\n",
    "column_names4 = ['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ']\n",
    "df4 = pd.read_table('Tyendinaga4_001.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=32, names=column_names4)\n",
    "#df4.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e15e217-4f62-4924-a2cc-2512b93b0332",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filename: Tyendinaga5\n",
    "column_names5 = ['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ']\n",
    "df5 = pd.read_table('Tyendinaga5_001.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=32, names=column_names5)\n",
    "#df5.head().style"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41bb9a69-7db8-4a78-9155-7ac6c9f0bdef",
   "metadata": {},
   "source": [
    "##### Check for null values:\n",
    "- will show \"False\" if no null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dec1c3f8-dfb6-4ee8-86df-ba1731f08fd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "any_null = df.isnull().any().any()\n",
    "print(\"df: \",any_null) # Will show false if no null values\n",
    "#df.info()\n",
    "print(\"df2:\", df2.isnull().any().any())\n",
    "print(\"df3:\", df3.isnull().any().any())\n",
    "print(\"df4:\", df4.isnull().any().any())\n",
    "print(\"df5:\", df5.isnull().any().any())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a79aecd9-3709-4a68-a42b-cdaf3960ae76",
   "metadata": {},
   "source": [
    "##### Columns of Interest:\n",
    "- NS: North-South component of motion\n",
    "- EW: East-West component of motion\n",
    "- Z: Vertical (Z-axis) component of motion\n",
    "\n",
    "##### Other Notes:\n",
    "- Sampling rate for all files: 1024 Hz\n",
    "- See below for preview of the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1cc36c-a570-4d2e-9bec-6177d479836f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(df.columns)\n",
    "# Want to make a time column:\n",
    "sampling_rate = 1024  # Hz (same for all data files)\n",
    "dataframes = [df, df2, df3, df4, df5]\n",
    "for i, dataframe in enumerate(dataframes):\n",
    "    if i==0:\n",
    "        print(\"dataframe: df\")\n",
    "    else:\n",
    "        print(f\"dataframe: df{i+1}\")\n",
    "    time = dataframe.index/sampling_rate\n",
    "    dataframe['time (s)'] = time\n",
    "    print(dataframe.head())\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "522c4daa-434f-42fb-a5c9-a68af5efbcb9",
   "metadata": {},
   "source": [
    "***\n",
    "### Plotting the Time Series\n",
    "##### Notes:\n",
    "- Plot the NS, EW, and Z data separately"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72202704-9f75-4244-876a-50861abc8514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder: dataframes = [df, df2, df3, df4, df5]\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i==0:\n",
    "        print('dataframe: df')\n",
    "    else:\n",
    "        print(f'dataframe: df{i+1}')\n",
    "\n",
    "    # Plot:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    axes[0].plot(time_minutes, dframe['NS'], label='North-South', color='forestgreen')\n",
    "    axes[0].set_ylabel('North-South Component')\n",
    "    axes[0].set_title('North-South Data')\n",
    "    axes[0].set_xlabel('Time (min)')\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # East-West data:\n",
    "    axes[1].plot(time_minutes, dframe['EW'], label='East-West', color='rebeccapurple')\n",
    "    axes[1].set_ylabel('East-West Component')\n",
    "    axes[1].set_title('East-West Data')\n",
    "    axes[1].set_xlabel('Time (min)')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    axes[2].plot(time_minutes, dframe['Z'], label='Z/Vertical Component', color='royalblue')\n",
    "    axes[2].set_ylabel('Z Component')\n",
    "    axes[2].set_title('Z-Component Data')\n",
    "    axes[2].set_xlabel('Time (min)')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab11572-97c8-48cb-aa40-60d2c19fd0de",
   "metadata": {},
   "source": [
    "***\n",
    "### Normalizing, Detrending and Shifting Data\n",
    "##### Notes:\n",
    "- data range of time series was normalized using MinMaxScaler from sklearn\n",
    "- data was linearly detrended using signal.detrend from scipy\n",
    "- the mean of the data was shifted to zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff0da38-7d1c-4e5f-ae72-2f4fc2295153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Linearly detrend data, normalize and shift to zero mean:\n",
    "# reminder: dataframes = [df, df2, df3, df4, df5]\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i==0:\n",
    "        print('dataframe: df')\n",
    "    else:\n",
    "        print(f'dataframe: df{i+1}')\n",
    "        \n",
    "    # Detrend\n",
    "    dframe['NS_Detrended'] = signal.detrend(dframe['NS'], type='linear') # North-South data\n",
    "    dframe['EW_Detrended'] = signal.detrend(dframe['EW'], type='linear') # East-West data\n",
    "    dframe['Z_Detrended'] = signal.detrend(dframe['Z'], type='linear') # Z-component data\n",
    "\n",
    "    # Normalize\n",
    "    scaler = MinMaxScaler()\n",
    "    dframe['NS_Normalized'] = scaler.fit_transform(dframe['NS_Detrended'].values.reshape(-1, 1)).flatten() # North-South\n",
    "    dframe['EW_Normalized'] = scaler.fit_transform(dframe['EW_Detrended'].values.reshape(-1, 1)).flatten() # East-West\n",
    "    dframe['Z_Normalized'] = scaler.fit_transform(dframe['Z_Detrended'].values.reshape(-1, 1)).flatten() # Z-component\n",
    "    print(f\"NS Min-Max normalized range: [{dframe['NS_Normalized'].min():.3f}, {dframe['NS_Normalized'].max():.3f}]\")\n",
    "    print(f\"EW Min-Max normalized range: [{dframe['EW_Normalized'].min():.3f}, {dframe['EW_Normalized'].max():.3f}]\")\n",
    "    print(f\"Z Min-Max normalized range: [{dframe['Z_Normalized'].min():.3f}, {dframe['Z_Normalized'].max():.3f}]\")\n",
    "\n",
    "    # Shift Mean:\n",
    "    dframe['NS_Normalized_Shifted'] = (dframe['NS_Normalized'] - np.mean(dframe['NS_Normalized']))\n",
    "    dframe['EW_Normalized_Shifted'] = (dframe['EW_Normalized'] - np.mean(dframe['EW_Normalized']))\n",
    "    dframe['Z_Normalized_Shifted'] = (dframe['Z_Normalized'] - np.mean(dframe['Z_Normalized']))\n",
    "    print(f\"New range for NS: [{dframe['NS_Normalized_Shifted'].min():.3f}, {dframe['NS_Normalized_Shifted'].max():.3f}]\")\n",
    "    print(f\"New mean for NS: {dframe['NS_Normalized_Shifted'].mean():.6f}\")\n",
    "    print(f\"New range for EW: [{dframe['EW_Normalized_Shifted'].min():.3f}, {dframe['EW_Normalized_Shifted'].max():.3f}]\")\n",
    "    print(f\"New mean for EW: {dframe['EW_Normalized_Shifted'].mean():.6f}\")\n",
    "    print(f\"New range for Z: [{dframe['Z_Normalized_Shifted'].min():.3f}, {dframe['Z_Normalized_Shifted'].max():.3f}]\")\n",
    "    print(f\"New mean for Z: {dframe['Z_Normalized_Shifted'].mean():.6f}\")\n",
    "\n",
    "    # Plot:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    axes[0].plot(time_minutes, dframe['NS_Normalized_Shifted'], label='North-South', color='forestgreen')\n",
    "    axes[0].set_ylabel('North-South Component')\n",
    "    axes[0].set_title('DNS North-South Data')\n",
    "    axes[0].set_xlabel('Time (min)')\n",
    "    axes[0].grid(True)\n",
    "\n",
    "    # East-West data:\n",
    "    axes[1].plot(time_minutes, dframe['EW_Normalized_Shifted'], label='East-West', color='rebeccapurple')\n",
    "    axes[1].set_ylabel('East-West Component')\n",
    "    axes[1].set_title('DNS East-West Data')\n",
    "    axes[1].set_xlabel('Time (min)')\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    axes[2].plot(time_minutes, dframe['Z_Normalized_Shifted'], label='Z/Vertical Component', color='royalblue')\n",
    "    axes[2].set_ylabel('Z Component')\n",
    "    axes[2].set_title('DNS Z-Component Data')\n",
    "    axes[2].set_xlabel('Time (min)')\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"DNS = Detrended, Normalized, Shifted\")\n",
    "    print(\"\")\n",
    "    # Stats for plots\n",
    "    print(\"North-South Stats:\")\n",
    "    print(ss.describe(dframe['NS_Normalized_Shifted']))\n",
    "    print(\"median:\", dframe[\"NS_Normalized_Shifted\"].median())\n",
    "    print(\"mode:\", ss.mode(df[\"NS_Normalized_Shifted\"])) \n",
    "    print(\"standard deviation:\", dframe[\"NS_Normalized_Shifted\"].std())\n",
    "    print(\"\")\n",
    "    print(\"East-West Stats:\")\n",
    "    print(ss.describe(dframe['EW_Normalized_Shifted']))\n",
    "    print(\"median:\", dframe[\"EW_Normalized_Shifted\"].median())\n",
    "    print(\"mode:\", ss.mode(dframe[\"EW_Normalized_Shifted\"])) \n",
    "    print(\"standard deviation:\", dframe[\"EW_Normalized_Shifted\"].std())\n",
    "    print(\"\")\n",
    "    print(\"Z-component Stats:\")\n",
    "    print(ss.describe(dframe['Z_Normalized_Shifted']))\n",
    "    print(\"median:\", dframe[\"Z_Normalized_Shifted\"].median())\n",
    "    print(\"mode:\", ss.mode(dframe[\"Z_Normalized_Shifted\"])) \n",
    "    print(\"standard deviation:\", dframe[\"Z_Normalized_Shifted\"].std())\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da29531b-aa23-4010-9768-cc22096142e7",
   "metadata": {},
   "source": [
    "***\n",
    "### Subsampling the Time Series\n",
    "##### Notes:\n",
    "- the original sampling rate for the data is 1024 Hz\n",
    "- different step sizes were applied to the normalized, detrended and mean-shifted (DNS) data for subsampling\n",
    "- adjust x-axis limits to focus in on certain times"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ffdc5f-6fe9-4988-884f-ca0b3fba4576",
   "metadata": {},
   "source": [
    "#### \"percent_check\" Function for Choosing Subsampling Step Size:\n",
    "##### Notes:\n",
    "- this method uses interpolation to compare subsampled data to the full DNS data\n",
    "- the absolute error between the full DNS data and the reconstructed subsampled data is calculated and divided by the full data range\n",
    "- this result is then used determine if the error is within the selected threshold (i.e. 5%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "904cc0c1-b70c-41bf-9228-239e60cd40d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# First use percent_check function to choose step-size (i.e. decimation_factor) to use for subsample function\n",
    "def percent_check(data, column, step, tolerance=0.05):\n",
    "    \"\"\"Determine if error for subsampled data is within 5% tolerance compared to full DNS data range\n",
    "    data: dataframe to use\n",
    "    column: column of interest for y values\n",
    "    step: data step to use for subsampling\n",
    "    tolerance: error tolerance expressed as a decimal\"\"\"\n",
    "    # Get original series\n",
    "    x_original = np.arange(len(data))\n",
    "    y_original = data[column].values\n",
    "    \n",
    "    # Get subsampled series\n",
    "    x_subsampled = np.arange(0, len(data), step)\n",
    "    y_subsampled = data[column].iloc[::step].values\n",
    "    \n",
    "    # Interpolate subsampled data back to original timestamps\n",
    "    f_interp = interpolate.interp1d(x_subsampled, y_subsampled, kind='linear', fill_value='extrapolate')\n",
    "    y_reconstructed = f_interp(x_original)\n",
    "    \n",
    "    # Calculate absolute errors\n",
    "    abs_errors = np.abs(y_original - y_reconstructed)\n",
    "    \n",
    "    # Calculate data range (the scale of the data)\n",
    "    data_range = np.max(y_original) - np.min(y_original)\n",
    "    #print(\"data_range:\", data_range)\n",
    "    # Normalize errors by data range instead of individual values\n",
    "    normalized_errors = abs_errors / data_range\n",
    "    \n",
    "    # Test if within tolerance\n",
    "    within_tolerance = normalized_errors <= tolerance\n",
    "    pct_within = (np.sum(within_tolerance) / len(within_tolerance)) * 100\n",
    "    \n",
    "    # Overall pass/fail\n",
    "    passes = pct_within >= 95  # At least 95% of points within tolerance\n",
    "    \n",
    "    results = {'step': step,\n",
    "        'tolerance': f'{tolerance*100}%',\n",
    "        'passes': passes,\n",
    "        'points_within_tolerance': f'{pct_within: .4f}%',\n",
    "        'num_original_points': len(y_original),\n",
    "        'num_subsampled_points': len(y_subsampled),\n",
    "        'data_range': f'{data_range: .4f}',  \n",
    "        'max_normalized_error': f'{(np.max(normalized_errors)*100): .4f}%' }\n",
    "    \n",
    "    return results\n",
    "# step_1 = 12\n",
    "# Results for df:\n",
    "NS_results_step_1 = percent_check(df, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS results 1 (df):\", NS_results_step_1)\n",
    "EW_results_step_1 = percent_check(df, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW results 1 (df):\", EW_results_step_1)\n",
    "Z_results_step_1 = percent_check(df, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z results 1 (df):\", Z_results_step_1)\n",
    "print(\"\")\n",
    "# Results for df2:\n",
    "NS_results_step_1_df2 = percent_check(df2, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS results 1 (df2):\", NS_results_step_1_df2)\n",
    "EW_results_step_1_df2 = percent_check(df2, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW results 1 (df2):\", EW_results_step_1_df2)\n",
    "Z_results_step_1_df2 = percent_check(df2, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z results 1 (df2):\", Z_results_step_1_df2)\n",
    "print(\"\")\n",
    "# Results for df3:\n",
    "NS_results_step_1_df3 = percent_check(df3, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS results 1 (df3):\", NS_results_step_1_df3)\n",
    "EW_results_step_1_df3 = percent_check(df3, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW results 1 (df3):\", EW_results_step_1_df3)\n",
    "Z_results_step_1_df3 = percent_check(df3, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z results 1 (df3):\", Z_results_step_1_df3)\n",
    "print(\"\")\n",
    "# Results for df4:\n",
    "NS_results_step_1_df4 = percent_check(df4, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS results 1 (df4):\", NS_results_step_1_df4)\n",
    "EW_results_step_1_df4 = percent_check(df4, 'EW_Normalized_Shifted', 7, tolerance=0.05) # passes with stepsizeof 10 but not 12\n",
    "print(\"EW results 1 (df4):\", EW_results_step_1_df4)\n",
    "Z_results_step_1_df4 = percent_check(df4, 'Z_Normalized_Shifted', 7, tolerance=0.05) # Z passes check with stepsize of 12 but not 10\n",
    "print(\"Z results 1 (df4):\", Z_results_step_1_df4)\n",
    "print(\"\")\n",
    "# Results for df5:\n",
    "NS_results_step_1_df5 = percent_check(df5, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS results 1 (df5):\", NS_results_step_1_df5)\n",
    "EW_results_step_1_df5 = percent_check(df5, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW results 1 (df5):\", EW_results_step_1_df5)\n",
    "Z_results_step_1_df5 = percent_check(df5, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z results 1 (df5):\", Z_results_step_1_df5)\n",
    "print(\"\")\n",
    "# Note: 7 is the largest stepsize that passes the tolerance check everywhere (i.e. for all dataframes), simultaneously"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30f2038b-d384-476a-bcbe-bede23daf9e9",
   "metadata": {},
   "source": [
    "#### percent_check results:\n",
    "- A step size of 7 was the largest that passed the percent_check test for NS, EW, and Z data simultaneously for all dataframes (plotted below)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a0c56d-67e2-4055-8a81-79f49e7cb946",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting time series using percent_check step sizes to compare to full DNS plots\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i==0:\n",
    "        print('dataframe: df')\n",
    "    else:\n",
    "        print(f'dataframe: df{i+1}')\n",
    "    # Plot:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    axes[0].plot(time_minutes, dframe['NS_Normalized_Shifted'], label='North-South (Full)', color='red')\n",
    "    axes[0].plot(time_minutes[::7], dframe['NS_Normalized_Shifted'][::7], label='North-South (Step=7)', color='forestgreen')\n",
    "    axes[0].set_ylabel('North-South Component')\n",
    "    axes[0].set_title('Subsampled and Full DNS North-South Data')\n",
    "    axes[0].set_xlabel('Time (min)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # East-West data:\n",
    "    axes[1].plot(time_minutes, dframe['EW_Normalized_Shifted'], label='East-West (Full)', color='red')\n",
    "    axes[1].plot(time_minutes[::7], dframe['EW_Normalized_Shifted'][::7], label='East-West (Step=7)', color='rebeccapurple')\n",
    "    axes[1].set_ylabel('East-West Component')\n",
    "    axes[1].set_title('Subsampled and Full DNS East-West Data')\n",
    "    axes[1].set_xlabel('Time (min)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    axes[2].plot(time_minutes, dframe['Z_Normalized_Shifted'], label='Z (Full)', color='red')\n",
    "    axes[2].plot(time_minutes[::7], dframe['Z_Normalized_Shifted'][::7], label='Z (Step=7)', color='royalblue')\n",
    "    axes[2].set_ylabel('Z Component')\n",
    "    axes[2].set_title('Subsampled and Full DNS Z-Component Data')\n",
    "    axes[2].set_xlabel('Time (min)')\n",
    "    axes[2].legend()\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f32ca59d-6156-4e48-a3ae-e482404bfd23",
   "metadata": {},
   "source": [
    "#### percent_check function using scipy.signal.decimate for y_subsampled instead of indexing:\n",
    "- testing a different method for subsampling with the same step size (i.e. 7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "649a3b0b-f958-4ca1-9c4c-133b0dd13809",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to use percent_check for scipy decimate version of subsampling\n",
    "def percent_check_scipy_decimate(data, column, step, tolerance=0.05):\n",
    "    x_original = np.arange(len(data))\n",
    "    y_original = data[column].values\n",
    "    \n",
    "    # Get subsampled series\n",
    "    x_subsampled = np.arange(0, len(data), step)\n",
    "    y_subsampled=scipy.signal.decimate(data[column].values, step, ftype='iir', zero_phase=True)\n",
    "    \n",
    "    # Interpolate subsampled data back to original timestamps\n",
    "    f_interp = interpolate.interp1d(x_subsampled, y_subsampled, kind='linear', fill_value='extrapolate')\n",
    "    y_reconstructed = f_interp(x_original)\n",
    "    \n",
    "    # Calculate absolute errors\n",
    "    abs_errors = np.abs(y_original - y_reconstructed)\n",
    "    \n",
    "    # Calculate data range (the scale of the data)\n",
    "    data_range = np.max(y_original) - np.min(y_original)\n",
    "    #print(\"data_range:\", data_range)\n",
    "    # Normalize errors by data range instead of individual values\n",
    "    normalized_errors = abs_errors / data_range\n",
    "    \n",
    "    # Test if within tolerance\n",
    "    within_tolerance = normalized_errors <= tolerance\n",
    "    pct_within = (np.sum(within_tolerance) / len(within_tolerance)) * 100\n",
    "    \n",
    "    # Overall pass/fail\n",
    "    passes = pct_within >= 95  # At least 95% of points within tolerance\n",
    "    \n",
    "    results = {'step': step,\n",
    "        'tolerance': f'{tolerance*100}%',\n",
    "        'passes': passes,\n",
    "        'points_within_tolerance': f'{pct_within: .4f}%',\n",
    "        'num_original_points': len(y_original),\n",
    "        'num_subsampled_points': len(y_subsampled),\n",
    "        'data_range': f'{data_range: .4f}',  \n",
    "        'max_normalized_error': f'{(np.max(normalized_errors)*100): .4f}%' }\n",
    "    return results\n",
    "\n",
    "# results for df:\n",
    "NS_results_test_1_df = percent_check_scipy_decimate(df, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS_results_test_1 (df):\", NS_results_test_1_df)\n",
    "EW_results_test_1_df = percent_check_scipy_decimate(df, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW_results_test_1 (df):\", EW_results_test_1_df)\n",
    "Z_results_test_1_df = percent_check_scipy_decimate(df, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z_results_test_1 (df):\", Z_results_test_1_df)\n",
    "print(\"\")\n",
    "# results for df2:\n",
    "NS_results_test_1_df2 = percent_check_scipy_decimate(df2, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS_results_test_1 (df2):\", NS_results_test_1_df2)\n",
    "EW_results_test_1_df2 = percent_check_scipy_decimate(df2, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW_results_test_1 (df2):\", EW_results_test_1_df2)\n",
    "Z_results_test_1_df2 = percent_check_scipy_decimate(df2, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z_results_test_1 (df2):\", Z_results_test_1_df2)\n",
    "print(\"\")\n",
    "# results for df3:\n",
    "NS_results_test_1_df3 = percent_check_scipy_decimate(df3, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS_results_test_1 (df3):\", NS_results_test_1_df3)\n",
    "EW_results_test_1_df3 = percent_check_scipy_decimate(df3, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW_results_test_1 (df3):\", EW_results_test_1_df3)\n",
    "Z_results_test_1_df3 = percent_check_scipy_decimate(df3, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z_results_test_1 (df3):\", Z_results_test_1_df3)\n",
    "print(\"\")\n",
    "# results for df4:\n",
    "NS_results_test_1_df4 = percent_check_scipy_decimate(df4, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS_results_test_1 (df4):\", NS_results_test_1_df4)\n",
    "EW_results_test_1_df4 = percent_check_scipy_decimate(df4, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW_results_test_1 (df4):\", EW_results_test_1_df4)\n",
    "Z_results_test_1_df4 = percent_check_scipy_decimate(df4, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z_results_test_1 (df4):\", Z_results_test_1_df4)\n",
    "print(\"\")\n",
    "# results for df5:\n",
    "NS_results_test_1_df5 = percent_check_scipy_decimate(df5, 'NS_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"NS_results_test_1 (df5):\", NS_results_test_1_df5)\n",
    "EW_results_test_1_df5 = percent_check_scipy_decimate(df5, 'EW_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"EW_results_test_1 (df5):\", EW_results_test_1_df5)\n",
    "Z_results_test_1_df5 = percent_check_scipy_decimate(df5, 'Z_Normalized_Shifted', 7, tolerance=0.05)\n",
    "print(\"Z_results_test_1 (df5):\", Z_results_test_1_df5)\n",
    "print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8abeabeb-81f7-4a00-b92b-a33e62a5f790",
   "metadata": {},
   "outputs": [],
   "source": [
    "# try using scipy.signal.decimate and compare to indexing results\n",
    "original_sampling_rate = 1024 # Hz\n",
    "def subsample(data, decimation_factor, sr):\n",
    "    \"\"\"Subsample data using scipy.signal.decimate with anti-aliasing filter.\n",
    "    data: input data (array)\n",
    "    decimation_factor: factor by which to reduce sampling rate\n",
    "    sr: Original sampling rate\"\"\"\n",
    "    # scipy.signal.decimate applies an anti-aliasing filter automatically\n",
    "    # Use a higher order filter for better anti-aliasing (default is 8)\n",
    "    decimated_data = scipy.signal.decimate(data, decimation_factor, ftype='iir', zero_phase=True)  \n",
    "    # get new sampling rate:\n",
    "    new_sr = sr / decimation_factor\n",
    "    #print(f\"New sampling rate: {new_sr} Hz\")\n",
    "    return decimated_data, new_sr\n",
    "\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Compute results for NS, EW, and Z data:\n",
    "# (Using stepsize of 7 for decimation_factor)\n",
    "# df:\n",
    "NS_sub_results_df, NS_new_sr_df = subsample(df['NS_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "EW_sub_results_df, EW_new_sr_df = subsample(df['EW_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "Z_sub_results_df, Z_new_sr_df = subsample(df['Z_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "#df2:\n",
    "NS_sub_results_df2, NS_new_sr_df2 = subsample(df2['NS_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "EW_sub_results_df2, EW_new_sr_df2 = subsample(df2['EW_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "Z_sub_results_df2, Z_new_sr_df2 = subsample(df2['Z_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "#df3:\n",
    "NS_sub_results_df3, NS_new_sr_df3 = subsample(df3['NS_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "EW_sub_results_df3, EW_new_sr_df3 = subsample(df3['EW_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "Z_sub_results_df3, Z_new_sr_df3 = subsample(df3['Z_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "#df4:\n",
    "NS_sub_results_df4, NS_new_sr_df4 = subsample(df4['NS_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "EW_sub_results_df4, EW_new_sr_df4 = subsample(df4['EW_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "Z_sub_results_df4, Z_new_sr_df4 = subsample(df4['Z_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "#df5:\n",
    "NS_sub_results_df5, NS_new_sr_df5 = subsample(df5['NS_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "EW_sub_results_df5, EW_new_sr_df5 = subsample(df5['EW_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "Z_sub_results_df5, Z_new_sr_df5 = subsample(df5['Z_Normalized_Shifted'].values, 7, original_sampling_rate)\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "NS_subsampled_data = [NS_sub_results_df, NS_sub_results_df2, NS_sub_results_df3, NS_sub_results_df4, NS_sub_results_df5]\n",
    "EW_subsampled_data = [EW_sub_results_df, EW_sub_results_df2, EW_sub_results_df3, EW_sub_results_df4, EW_sub_results_df5]\n",
    "Z_subsampled_data = [Z_sub_results_df, Z_sub_results_df2, Z_sub_results_df3, Z_sub_results_df4, Z_sub_results_df5]\n",
    "\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    # Plot:\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(time_minutes, dframe['NS_Normalized_Shifted'], label='North-South (Full)', color='red')\n",
    "    plt.plot(time_minutes[::7], NS_subsampled_data[i], label='North-South (Step=7)', color='forestgreen')\n",
    "    plt.ylabel('North-South Component')\n",
    "    plt.title('Subsampled and Full DNS North-South Data')\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # East-West data:\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(time_minutes, dframe['EW_Normalized_Shifted'], label='East-West (Full)', color='red')\n",
    "    plt.plot(time_minutes[::7], EW_subsampled_data[i], label='East-West (Step=7)', color='rebeccapurple')\n",
    "    plt.ylabel('East-West Component')\n",
    "    plt.title('Subsampled and Full DNS East-West Data')\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    plt.figure(figsize=(12,4))\n",
    "    plt.plot(time_minutes, dframe['Z_Normalized_Shifted'], label='Z (Full)', color='red')\n",
    "    plt.plot(time_minutes[::7], Z_subsampled_data[i], label='Z (Step=7)', color='royalblue')\n",
    "    plt.ylabel('Z Component')\n",
    "    plt.title('Subsampled and Full DNS Z-Component Data')\n",
    "    plt.xlabel('Time (min)')\n",
    "    plt.legend(loc='upper left')\n",
    "    plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "# Note: New sampling rate is 146.285714... Hz using decimation_factor of 7"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "92dc7f01-ef08-49e2-b383-5f5edcec5acb",
   "metadata": {},
   "source": [
    "#### Comparing percent_check results from scipy.signal.decimate and indexing for y_subsampled\n",
    "- There were a similar percentage of points within the error tolerance when using the scipy.signal.decimate method compared to the indexing method for subsampling\n",
    "- For later cells, the scipy.signal.decimate method will be used as it includes a built-in anti-aliasing filter"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7cdea4f-9ff0-431e-bc67-6b8b3eed6afa",
   "metadata": {},
   "source": [
    "***\n",
    "### Tapering the Time Series\n",
    "##### Notes:\n",
    "- a Hann window/raised cosine was used to taper the DNS time series\n",
    "- the data is tapered starting at 10% from the edges (this percentage can be adjusted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e93d8297-141b-4f2a-9f4c-3d1e903bb47e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reminder (defined in previous cell): dataframes = [df, df2, df3, df4, df5]\n",
    "def apply_hann_window(data, NS_column, EW_column, Z_column, taper_fraction = 0.1):\n",
    "    \"\"\"Taper edges of time series with Hann window\n",
    "    taper_fraction = 0.1 will start taper at 10% from edges\"\"\"\n",
    "    n= len(data)\n",
    "    taper_length = int(n * taper_fraction) # should be same for all components\n",
    "\n",
    "    # NS Data:\n",
    "    # Create a copy of the data\n",
    "    NS_windowed_data = data[NS_column].copy()\n",
    "    NS_windowed_data = NS_windowed_data.astype(float) # needed for FFT code to work properly later on\n",
    "    # Apply Hann window to the left edge\n",
    "    NS_left_window = np.hanning(2 * taper_length)[:taper_length]\n",
    "    NS_windowed_data.iloc[:taper_length] *= NS_left_window\n",
    "    # Apply Hann window to the right edge\n",
    "    NS_right_window = np.hanning(2 * taper_length)[taper_length:]\n",
    "    NS_windowed_data.iloc[-taper_length:] *= NS_right_window\n",
    "\n",
    "    # EW Data:\n",
    "    # Create a copy of the data\n",
    "    EW_windowed_data = data[EW_column].copy()\n",
    "    EW_windowed_data = EW_windowed_data.astype(float) \n",
    "    # Apply Hann window to the left edge\n",
    "    EW_left_window = np.hanning(2 * taper_length)[:taper_length]\n",
    "    EW_windowed_data.iloc[:taper_length] *= EW_left_window\n",
    "    # Apply Hann window to the right edge\n",
    "    EW_right_window = np.hanning(2 * taper_length)[taper_length:]\n",
    "    EW_windowed_data.iloc[-taper_length:] *= EW_right_window\n",
    "\n",
    "    # Z Data:\n",
    "    Z_windowed_data = data[Z_column].copy()\n",
    "    Z_windowed_data = Z_windowed_data.astype(float) \n",
    "    # Apply Hann window to the left edge\n",
    "    Z_left_window = np.hanning(2 * taper_length)[:taper_length]\n",
    "    Z_windowed_data.iloc[:taper_length] *= Z_left_window\n",
    "    # Apply Hann window to the right edge\n",
    "    Z_right_window = np.hanning(2 * taper_length)[taper_length:]\n",
    "    Z_windowed_data.iloc[-taper_length:] *= Z_right_window\n",
    "    \n",
    "    return NS_windowed_data, EW_windowed_data, Z_windowed_data\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# df:\n",
    "NS_hann_result_df, EW_hann_result_df, Z_hann_result_df = apply_hann_window(df, 'NS_Normalized_Shifted', 'EW_Normalized_Shifted', 'Z_Normalized_Shifted')\n",
    "# df2:\n",
    "NS_hann_result_df2, EW_hann_result_df2, Z_hann_result_df2 = apply_hann_window(df2, 'NS_Normalized_Shifted', 'EW_Normalized_Shifted', 'Z_Normalized_Shifted')\n",
    "# df3:\n",
    "NS_hann_result_df3, EW_hann_result_df3, Z_hann_result_df3 = apply_hann_window(df3, 'NS_Normalized_Shifted', 'EW_Normalized_Shifted', 'Z_Normalized_Shifted')\n",
    "# df4:\n",
    "NS_hann_result_df4, EW_hann_result_df4, Z_hann_result_df4 = apply_hann_window(df4, 'NS_Normalized_Shifted', 'EW_Normalized_Shifted', 'Z_Normalized_Shifted')\n",
    "# df5:\n",
    "NS_hann_result_df5, EW_hann_result_df5, Z_hann_result_df5 = apply_hann_window(df5, 'NS_Normalized_Shifted', 'EW_Normalized_Shifted', 'Z_Normalized_Shifted')\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "NS_hann_results = [NS_hann_result_df, NS_hann_result_df2, NS_hann_result_df3, NS_hann_result_df4, NS_hann_result_df5]\n",
    "EW_hann_results = [EW_hann_result_df, EW_hann_result_df2, EW_hann_result_df3, EW_hann_result_df4, EW_hann_result_df5]\n",
    "Z_hann_results = [Z_hann_result_df, Z_hann_result_df2, Z_hann_result_df3, Z_hann_result_df4, Z_hann_result_df5]\n",
    "\n",
    "# Plot tapered data with full data for all dataframes:\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i == 0:\n",
    "        print(\"dataframe: df\")\n",
    "    else:\n",
    "        print(f\"dataframe: df{i+1}\")\n",
    "    # Plot:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(10, 10))\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    axes[0].plot(time_minutes, dframe['NS_Normalized_Shifted'], label='North-South (Full)', color='red')\n",
    "    axes[0].plot(time_minutes, NS_hann_results[i], label='North-South (Tapered)', color='forestgreen')\n",
    "    axes[0].set_ylabel('North-South Component')\n",
    "    axes[0].set_title('Tapered and Full DNS North-South Data')\n",
    "    axes[0].set_xlabel('Time (min)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # East-West data:\n",
    "    axes[1].plot(time_minutes, dframe['EW_Normalized_Shifted'], label='East-West (Full)', color='red')\n",
    "    axes[1].plot(time_minutes, EW_hann_results[i], label='East-West (Tapered)', color='rebeccapurple')\n",
    "    axes[1].set_ylabel('East-West Component')\n",
    "    axes[1].set_title('Tapered and Full DNS East-West Data')\n",
    "    axes[1].set_xlabel('Time (min)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    axes[2].plot(time_minutes, dframe['Z_Normalized_Shifted'], label='Z (Full)', color='red')\n",
    "    axes[2].plot(time_minutes, Z_hann_results[i], label='Z (Tapered)', color='royalblue')\n",
    "    axes[2].set_ylabel('Z Component')\n",
    "    axes[2].set_title('Tapered and Full DNS Z-Component Data')\n",
    "    axes[2].set_xlabel('Time (min)')\n",
    "    axes[2].legend()\n",
    "    #axes[2].set_xlim(-0.001,0.01)\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86f1099f-3ce2-4697-a5af-823ec6ecadb2",
   "metadata": {},
   "source": [
    "#### 'percent_check_tapered' for tapered time series:\n",
    "- a tapering version of the 'percent_check' function that was used for subsampling\n",
    "- based on tapering starting at 10% from the data edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48e08a86-81f0-46dd-99aa-20c38df407be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def percent_check_tapered(original_data, tapered_data_full, padding, tolerance=0.05):\n",
    "    \"\"\"original_data : Original unpadded time series (array)\n",
    "    tapered_data_full : Tapered time series, includes padding (array)\n",
    "    padding : Amount of padding added to each end (int)\n",
    "    tolerance : Acceptable error as fraction of data range (float)\"\"\"\n",
    "    # Extract original from tapered data (remove padding)\n",
    "    if padding > 0:\n",
    "        tapered_original_region = tapered_data_full[padding:-padding]\n",
    "    else:\n",
    "        tapered_original_region = tapered_data_full\n",
    "    \n",
    "    # Direct comparison: tapered vs original\n",
    "    abs_errors = np.abs(original_data - tapered_original_region)\n",
    "    # normalization\n",
    "    data_range = np.max(original_data) - np.min(original_data)\n",
    "    normalized_errors = abs_errors / data_range\n",
    "    \n",
    "    # Calculate metrics\n",
    "    within_tolerance = normalized_errors <= tolerance\n",
    "    pct_within = (np.sum(within_tolerance) / len(within_tolerance))*100\n",
    "    \n",
    "    # Mean Absolute Error (MAE) metrics\n",
    "    mae = np.mean(abs_errors)\n",
    "    std_original = np.std(original_data)\n",
    "    mae_pct_std = (mae / std_original)*100\n",
    "    \n",
    "    passes = pct_within >= 95\n",
    "    \n",
    "    results = {\n",
    "        'padding': padding,\n",
    "        'tolerance': f'{tolerance*100}%',\n",
    "        'passes': passes,\n",
    "        'points_within_tolerance': f'{pct_within: .4f}%',\n",
    "        'max_normalized_error': f'{(np.max(normalized_errors)*100): .4f}%',\n",
    "        'mean_abs_error': f'{mae: .4f}',\n",
    "        'mae_as_%_of_std': f'{mae_pct_std: .4f}%',\n",
    "        'num_points': len(original_data),\n",
    "        'data_range': f'{data_range: .4f}'}\n",
    "    \n",
    "    return results\n",
    "# df:\n",
    "NS_taper_results_1_df = percent_check_tapered(df['NS_Normalized_Shifted'], NS_hann_result_df, padding=0, tolerance=0.05)\n",
    "print(\"NS_taper_results_1 (df):\", NS_taper_results_1_df)\n",
    "EW_taper_results_1_df = percent_check_tapered(df['EW_Normalized_Shifted'], EW_hann_result_df, padding=0, tolerance=0.05)\n",
    "print(\"EW_taper_results_1 (df):\", EW_taper_results_1_df)\n",
    "Z_taper_results_1_df = percent_check_tapered(df['Z_Normalized_Shifted'], Z_hann_result_df, padding=0, tolerance=0.05)\n",
    "print(\"Z_taper_results_1 (df):\", Z_taper_results_1_df)\n",
    "print(\"\")\n",
    "# df2:\n",
    "NS_taper_results_1_df2 = percent_check_tapered(df2['NS_Normalized_Shifted'], NS_hann_result_df2, padding=0, tolerance=0.05)\n",
    "print(\"NS_taper_results_1 (df2):\", NS_taper_results_1_df2)\n",
    "EW_taper_results_1_df2 = percent_check_tapered(df2['EW_Normalized_Shifted'], EW_hann_result_df2, padding=0, tolerance=0.05)\n",
    "print(\"EW_taper_results_1 (df2):\", EW_taper_results_1_df2)\n",
    "Z_taper_results_1_df2 = percent_check_tapered(df2['Z_Normalized_Shifted'], Z_hann_result_df2, padding=0, tolerance=0.05)\n",
    "print(\"Z_taper_results_1 (df2):\", Z_taper_results_1_df2)\n",
    "print(\"\")\n",
    "# df3:\n",
    "NS_taper_results_1_df3 = percent_check_tapered(df3['NS_Normalized_Shifted'], NS_hann_result_df3, padding=0, tolerance=0.05)\n",
    "print(\"NS_taper_results_1 (df3):\", NS_taper_results_1_df3)\n",
    "EW_taper_results_1_df3 = percent_check_tapered(df3['EW_Normalized_Shifted'], EW_hann_result_df3, padding=0, tolerance=0.05)\n",
    "print(\"EW_taper_results_1 (df3):\", EW_taper_results_1_df3)\n",
    "Z_taper_results_1_df3 = percent_check_tapered(df3['Z_Normalized_Shifted'], Z_hann_result_df3, padding=0, tolerance=0.05)\n",
    "print(\"Z_taper_results_1 (df3):\", Z_taper_results_1_df3)\n",
    "print(\"\")\n",
    "# df4:\n",
    "NS_taper_results_1_df4 = percent_check_tapered(df4['NS_Normalized_Shifted'], NS_hann_result_df4, padding=0, tolerance=0.05)\n",
    "print(\"NS_taper_results_1 (df4):\", NS_taper_results_1_df4)\n",
    "EW_taper_results_1_df4 = percent_check_tapered(df4['EW_Normalized_Shifted'], EW_hann_result_df4, padding=0, tolerance=0.05)\n",
    "print(\"EW_taper_results_1 (df4):\", EW_taper_results_1_df4)\n",
    "Z_taper_results_1_df4 = percent_check_tapered(df4['Z_Normalized_Shifted'], Z_hann_result_df4, padding=0, tolerance=0.05)\n",
    "print(\"Z_taper_results_1 (df4):\", Z_taper_results_1_df4)\n",
    "print(\"\")\n",
    "# df5:\n",
    "NS_taper_results_1_df5 = percent_check_tapered(df5['NS_Normalized_Shifted'], NS_hann_result_df5, padding=0, tolerance=0.05)\n",
    "print(\"NS_taper_results_1 (df5):\", NS_taper_results_1_df5)\n",
    "EW_taper_results_1_df5 = percent_check_tapered(df5['EW_Normalized_Shifted'], EW_hann_result_df5, padding=0, tolerance=0.05)\n",
    "print(\"EW_taper_results_1 (df5):\", EW_taper_results_1_df5)\n",
    "Z_taper_results_1_df5 = percent_check_tapered(df5['Z_Normalized_Shifted'], Z_hann_result_df5, padding=0, tolerance=0.05)\n",
    "print(\"Z_taper_results_1 (df5):\", Z_taper_results_1_df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaf06d4b-4056-48e5-b32e-7b41e9332d3d",
   "metadata": {},
   "source": [
    "***\n",
    "### Combining Subsampling and Tapering\n",
    "##### Notes:\n",
    "- The full series was tapered before subsampling\n",
    "- A Hann window was used for tapering\n",
    "- A step size of 7 was used for subsampling, consistent across the North-South, East-West, and Up-Down (Z) data\n",
    "    - adjust the step size used for subsampling as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c91e5b5-ae74-4fee-b089-04bc7427f21f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Put tapered results into subsample function from a previous cell:\n",
    "def subsample_windowed(NS_windowed, EW_windowed, Z_windowed, sr=1024, decimation_factor=7):\n",
    "    NS_decimated_data = scipy.signal.decimate(NS_windowed, decimation_factor, ftype='iir', zero_phase=True)\n",
    "    EW_decimated_data = scipy.signal.decimate(EW_windowed, decimation_factor, ftype='iir', zero_phase=True)\n",
    "    Z_decimated_data = scipy.signal.decimate(Z_windowed, decimation_factor, ftype='iir', zero_phase=True)\n",
    "    new_sr = sr/decimation_factor\n",
    "    return NS_decimated_data, EW_decimated_data, Z_decimated_data, new_sr\n",
    "# Results for all dataframes:  ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# df:\n",
    "NS_decimated_data_df, EW_decimated_data_df, Z_decimated_data_df, new_sr_df = subsample_windowed(NS_hann_result_df, EW_hann_result_df, Z_hann_result_df)\n",
    "# df2:\n",
    "NS_decimated_data_df2, EW_decimated_data_df2, Z_decimated_data_df2, new_sr_df2 = subsample_windowed(NS_hann_result_df2, EW_hann_result_df2, Z_hann_result_df2)\n",
    "# df3:\n",
    "NS_decimated_data_df3, EW_decimated_data_df3, Z_decimated_data_df3, new_sr_df3 = subsample_windowed(NS_hann_result_df3, EW_hann_result_df3, Z_hann_result_df3)\n",
    "# df4:\n",
    "NS_decimated_data_df4, EW_decimated_data_df4, Z_decimated_data_df4, new_sr_df4 = subsample_windowed(NS_hann_result_df4, EW_hann_result_df4, Z_hann_result_df4)\n",
    "# df5:\n",
    "NS_decimated_data_df5, EW_decimated_data_df5, Z_decimated_data_df5, new_sr_df5 = subsample_windowed(NS_hann_result_df5, EW_hann_result_df5, Z_hann_result_df5)\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "NS_taper_sub = [NS_decimated_data_df, NS_decimated_data_df2, NS_decimated_data_df3, NS_decimated_data_df4, NS_decimated_data_df5]\n",
    "EW_taper_sub = [EW_decimated_data_df, EW_decimated_data_df2, EW_decimated_data_df3, EW_decimated_data_df4, EW_decimated_data_df5]\n",
    "Z_taper_sub = [Z_decimated_data_df, Z_decimated_data_df2, Z_decimated_data_df3, Z_decimated_data_df4, Z_decimated_data_df5]\n",
    "\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i ==0:\n",
    "        print(\"dataframe: df\")\n",
    "    else:\n",
    "        print(f\"dataframe: df{i+1}\")\n",
    "    # Plot:\n",
    "    fig, axes = plt.subplots(3, 1, figsize=(12, 12))\n",
    "    time_minutes = dframe['time (s)']/60\n",
    "    # North-South data:\n",
    "    axes[0].plot(time_minutes, dframe['NS_Normalized_Shifted'], label='North-South (Full)', color='red')\n",
    "    axes[0].plot(time_minutes[::7], NS_taper_sub[i], label='Tapered North-South (Step=7)', color='forestgreen') # make sure step for time_minutes=decimation_factor\n",
    "    axes[0].set_ylabel('North-South Component')\n",
    "    axes[0].set_title('Tapered/Subsampled and Full DNS North-South Data')\n",
    "    axes[0].set_xlabel('Time (min)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(True)\n",
    "    \n",
    "    # East-West data:\n",
    "    axes[1].plot(time_minutes, dframe['EW_Normalized_Shifted'], label='East-West (Full)', color='red')\n",
    "    axes[1].plot(time_minutes[::7], EW_taper_sub[i], label='Tapered East-West (Step=7)', color='rebeccapurple')\n",
    "    axes[1].set_ylabel('East-West Component')\n",
    "    axes[1].set_title('Tapered/Subsampled and Full DNS East-West Data')\n",
    "    axes[1].set_xlabel('Time (min)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(True)\n",
    "    \n",
    "    # Z data:\n",
    "    axes[2].plot(time_minutes, dframe['Z_Normalized_Shifted'], label='Z (Full)', color='red')\n",
    "    axes[2].plot(time_minutes[::7], Z_taper_sub[i], label='Tapered Z (Step=7)', color='royalblue')\n",
    "    axes[2].set_ylabel('Z Component')\n",
    "    axes[2].set_title('Tapered/Subsampled and Full DNS Z-Component Data')\n",
    "    axes[2].set_xlabel('Time (min)')\n",
    "    axes[2].legend()\n",
    "    #axes[2].set_xlim(-0.001,0.01)\n",
    "    axes[2].grid(True)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "075a4945-5824-4a0f-bbec-539cebc5c317",
   "metadata": {},
   "source": [
    "***\n",
    "### Fast Fourier Transform\n",
    "##### Notes:\n",
    "- Applying the fast fourier transform to subsampled and tapered DNS data and plotting the resulting magnitude and phase spectra\n",
    "    - (commented out phase spectra for now)\n",
    "    - also defined a coherent gain function in attempt to correct for the amplitude reduction from windowing<br>\n",
    "<br> Effects of subsampling/tapering:\n",
    "    - removes higher frequency content\n",
    "    - reduces spectral leakage caused by discontinuities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ca8cbe-cf81-4d70-ba49-35f5e80a600a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hann window one column at a time:\n",
    "def apply_hann_window1(data, column, taper_fraction = 0.1):\n",
    "    \"\"\"Taper edges of time series with Hann window\n",
    "    taper_fraction = 0.1 will start taper at 10% from edges\"\"\"\n",
    "    n= len(data)\n",
    "    taper_length = int(n * taper_fraction)\n",
    "    \n",
    "    # Create a copy of the data\n",
    "    windowed_data = data[column].copy()\n",
    "    windowed_data = windowed_data.astype(float) # needed for FFT code to work properly\n",
    "    \n",
    "    # Apply Hann window to the left edge\n",
    "    left_window = np.hanning(2 * taper_length)[:taper_length]\n",
    "    windowed_data[:taper_length] *= left_window\n",
    "    \n",
    "    # Apply Hann window to the right edge\n",
    "    right_window = np.hanning(2 * taper_length)[taper_length:]\n",
    "    windowed_data[-taper_length:] *= right_window\n",
    "    \n",
    "    return windowed_data\n",
    "\n",
    "#print(type(apply_hann_window1(df, \"NS_Normalized_Shifted\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0561895-d184-4fe2-820d-1e96594eb7f2",
   "metadata": {},
   "source": [
    "##### Coherent gain equation:\n",
    "$G_c = \\frac{1}{N}\\sum_{n=0}^{N-1}\\omega[n]$\n",
    "\n",
    "- $G_c$: coherent gain\n",
    "- $\\omega[n]$: window\n",
    "- $N$: window length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4ffaea-717a-43b1-ac76-880bc1aea8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_coherent_gain(data, taper_fraction=0.1):\n",
    "    \"\"\"Calculate coherent gain for partial Hann taper\"\"\"\n",
    "    n=len(data)\n",
    "    taper_length = int(n * taper_fraction)\n",
    "    \n",
    "    # Create the full window\n",
    "    window = np.ones(n)\n",
    "    \n",
    "    # Left taper\n",
    "    left_window = np.hanning(2 * taper_length)[:taper_length]\n",
    "    window[:taper_length] = left_window\n",
    "    \n",
    "    # Right taper\n",
    "    right_window = np.hanning(2 * taper_length)[taper_length:]\n",
    "    window[-taper_length:] = right_window\n",
    "    \n",
    "    # Coherent gain is the mean\n",
    "    coherent_gain = np.mean(window)\n",
    "    \n",
    "    return coherent_gain\n",
    "\n",
    "# For taper_fraction = 0.1\n",
    "coherent_gain = calculate_coherent_gain(df['NS_Normalized_Shifted'], taper_fraction=0.1)\n",
    "#print(coherent_gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23ef0041-14b0-4753-9d02-494b4a049f5c",
   "metadata": {},
   "source": [
    "#### Approach 1: Overlay FFT results for tapered/subsampled data onto single figure with full DNS results\n",
    "- plot for each dataframe as needed\n",
    "    - may need to convert subsampled data from arrays to dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5f87dc8-7c62-4a1d-936c-e6b78d69d093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# smoothing option: moving average\n",
    "# note to self: check\n",
    "def smooth_spectrum(magnitudes, window_size=5):\n",
    "    \"\"\"Apply moving average smoothing to FFT magnitudes\n",
    "    magnitudes: array of FFT magnitude values\n",
    "    window_size: number of points to average (must be odd for symmetry)\"\"\"\n",
    "    return np.convolve(magnitudes, np.ones(window_size)/window_size, mode='same')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbac3381-b2ff-4fb5-8cac-db4592cf182a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert subsampled data from arrays to DataFrames:\n",
    "NS_sub_results_dframe = pd.DataFrame(NS_sub_results_df, columns=[\"NS_subsampled\"])\n",
    "EW_sub_results_dframe = pd.DataFrame(EW_sub_results_df, columns=[\"EW_subsampled\"])\n",
    "Z_sub_results_dframe = pd.DataFrame(Z_sub_results_df, columns=[\"Z_subsampled\"])\n",
    "\n",
    "# Plotting FFT spectra showing tapered/subsampled data overlayed onto full DNS data for a given dataframe\n",
    "def plot_fft(data, column, sr, fig1=None, fig2=None, name='', apply_window=True, smooth=False, smooth_window=5):\n",
    "    \"\"\"Plot FFT with windowing correction.\n",
    "    data: dataframe of interest\n",
    "    column: column of interest from data\n",
    "    name: name of data column, string\n",
    "    sr: Sampling rate\n",
    "    apply_window: bool, says whether to apply Hann window (default True)\n",
    "    smooth: bool, says whether to apply moving average smoothing\n",
    "    smooth_window: size of smoothing window\"\"\"\n",
    "    n = len(data)\n",
    "    \n",
    "    # Apply window if requested\n",
    "    if apply_window:\n",
    "        windowed_data = apply_hann_window1(data, column, taper_fraction=0.1)\n",
    "        # Calculate coherent gain for partial taper\n",
    "        coherent_gain = calculate_coherent_gain(data, taper_fraction=0.1)\n",
    "    else:\n",
    "        windowed_data = data[column].values\n",
    "        coherent_gain = 1.0\n",
    "    \n",
    "    # Compute FFT\n",
    "    fft_values = scipy.fft.rfft(windowed_data)\n",
    "    freqs = scipy.fft.rfftfreq(n, d=1/sr)\n",
    "    \n",
    "    # magnitudes WITH coherent gain correction\n",
    "    magnitudes = np.abs(fft_values) / (n * coherent_gain)\n",
    "    \n",
    "    # Double the AC components (not DC and Nyquist)\n",
    "    magnitudes[1:] *= 2.0\n",
    "    \n",
    "    # Undo doubling for Nyquist if even length\n",
    "    if n % 2 == 0:\n",
    "        magnitudes[-1] /= 2.0\n",
    "\n",
    "    #phases= np.angle(fft_values) leaving out phases for now\n",
    "    \n",
    "    # Apply smoothing if requested\n",
    "    if smooth:\n",
    "        magnitudes = smooth_spectrum(magnitudes, window_size=smooth_window)\n",
    "    \n",
    "    # Create figures:\n",
    "    if fig1 is None:\n",
    "        fig1 = plt.figure(figsize=(10,6))\n",
    "        plt.plot(freqs, magnitudes, linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (Linear)', fontsize=16)\n",
    "        plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        plt.minorticks_on()\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    else:\n",
    "        plt.figure(fig1.number)\n",
    "        plt.plot(freqs, magnitudes, linewidth=1, label=name)\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    if fig2 is None:\n",
    "        fig2 = plt.figure(figsize=(10, 6))\n",
    "        plt.loglog(freqs[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (log)', fontsize=16)\n",
    "        plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='lower left')\n",
    "    else:\n",
    "        plt.figure(fig2.number)\n",
    "        plt.loglog(freqs[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.legend()\n",
    "        if name:\n",
    "            plt.legend(loc='lower left')\n",
    "    \n",
    "    # if fig3 is None:\n",
    "    #     fig3 = plt.figure(figsize=(10, 6))\n",
    "    #     plt.plot(freqs[1:], phases[1:], linewidth=1, label=name)\n",
    "    #     plt.title('Phase Spectrum', fontsize=16)\n",
    "    #     plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "    #     plt.ylabel('Phase (radians)', fontsize=14)\n",
    "    #     plt.grid(True, alpha=0.5)\n",
    "    #     plt.xticks(fontsize=14)\n",
    "    #     plt.yticks(fontsize=14)\n",
    "    #     #plt.xlim(-1, 50)\n",
    "    #     plt.tight_layout()\n",
    "    #     if name:\n",
    "    #         plt.legend(loc='lower right', framealpha=0.5)\n",
    "    # else:\n",
    "    #     plt.figure(fig3.number)\n",
    "    #     plt.plot(freqs[1:], phases[1:], linewidth=1, label=name)\n",
    "    #     if name:\n",
    "    #         plt.legend(loc='lower right', framealpha=0.8)\n",
    "    \n",
    "    return fig1, fig2#, fig3 \n",
    "#####################################################################################################################\n",
    "#Use to create figures for one dataframe at a time:\n",
    "print(\"dataframe: df\")\n",
    "#North-South for df:\n",
    "# For full NS data\n",
    "fig1, fig2 = plot_fft(df, 'NS_Normalized_Shifted', sr=original_sampling_rate, name='Full NS Data', apply_window=False, smooth=True, smooth_window=5)\n",
    "# For tapered/subsampled data \n",
    "plot_fft(NS_sub_results_dframe, 'NS_subsampled', new_sr_df, fig1=fig1, fig2=fig2, name='Subsampled/Tapered NS Data', apply_window=True,\n",
    "         smooth=True, smooth_window=5)\n",
    "plt.show()\n",
    "\n",
    "#East-West:\n",
    "# For full EW data\n",
    "fig1, fig2 = plot_fft(df, 'EW_Normalized_Shifted', sr=original_sampling_rate, name='Full EW Data', apply_window=False, smooth=True, smooth_window=5)\n",
    "# For tapered/subsampled data \n",
    "plot_fft(EW_sub_results_dframe, 'EW_subsampled', new_sr_df, fig1=fig1, fig2=fig2, name='Subsampled/Tapered EW Data', apply_window=True, \n",
    "         smooth=True, smooth_window=5)\n",
    "plt.show()\n",
    "\n",
    "#Up-Down:\n",
    "# For full Z data\n",
    "fig1, fig2 = plot_fft(df, 'Z_Normalized_Shifted', sr=original_sampling_rate, name='Full Z Data', apply_window=False, smooth=True, smooth_window=5)\n",
    "# For tapered/subsampled data \n",
    "plot_fft(Z_sub_results_dframe,'Z_subsampled', new_sr_df, fig1=fig1, fig2=fig2, name='Subsampled/Tapered Z Data',\n",
    "         apply_window=True, smooth=True, smooth_window=5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773d88a5-a137-4225-b487-6667b07acc02",
   "metadata": {},
   "source": [
    "### Horizontal and Vertical FFTs\n",
    "- Goal: Add NS and EW FFTs together to get one set of plots (Horizontal)\n",
    "- Leave Z FFT plots as is (Vertical), plotted from a previous cell"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66ec0bf8-04b0-4eb1-8044-508a1c9d3532",
   "metadata": {},
   "source": [
    "#### Horizontal FFT Plots:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74199ce6-0005-47a1-9954-9e751f8f62a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# overlays tapered/subsampled result onto full DNS result for given dataframe\n",
    "def plot_fft_horizontal(data_ns, column_ns, data_ew, column_ew, sr, fig1=None, fig2=None, name='', apply_window=True, smooth=False, smooth_window=5):\n",
    "    \"\"\"Plot horizontal FFT spectrum from North-South and East-West components.\n",
    "    data_ns: North-South dataframe with column: column_ns\n",
    "    data_ew: East-West dataframe with column: column_ew\n",
    "    sr: Sampling rate\n",
    "    name: data label for legend, string\n",
    "    apply_window: whether to apply hann window to input data, bool\n",
    "    smooth: whether to apply moving average smoothing\n",
    "    smooth_window: size of smoothing window\"\"\"\n",
    "    n = len(data_ns)\n",
    "    \n",
    "    # Apply window if requested\n",
    "    if apply_window:\n",
    "        windowed_data_ns = apply_hann_window1(data_ns, column_ns, taper_fraction=0.1)\n",
    "        windowed_data_ew = apply_hann_window1(data_ew, column_ew, taper_fraction=0.1)\n",
    "        \n",
    "        # Calculate coherent gain for partial taper:\n",
    "        coherent_gain = calculate_coherent_gain(data_ns[column_ns], taper_fraction=0.1)\n",
    "    else:\n",
    "        windowed_data_ns = data_ns[column_ns].values\n",
    "        windowed_data_ew = data_ew[column_ew].values\n",
    "        coherent_gain = 1.0\n",
    "    \n",
    "    # Compute FFT for both components\n",
    "    fft_ns = scipy.fft.rfft(windowed_data_ns)\n",
    "    fft_ew = scipy.fft.rfft(windowed_data_ew)\n",
    "    freqs = scipy.fft.rfftfreq(n, d=1/sr)\n",
    "    \n",
    "    # Compute magnitudes WITHOUT coherent gain correction\n",
    "    mag_ns = np.abs(fft_ns) / n\n",
    "    mag_ew = np.abs(fft_ew) / n\n",
    "    \n",
    "    # Double the AC components (not DC and Nyquist)\n",
    "    mag_ns[1:] *= 2.0\n",
    "    mag_ew[1:] *= 2.0\n",
    "    \n",
    "    # Undo doubling for Nyquist if even length\n",
    "    if n % 2 == 0:\n",
    "        mag_ns[-1] /= 2.0\n",
    "        mag_ew[-1] /= 2.0\n",
    "    \n",
    "    # Combine as RMS for horizontal magnitude?\n",
    "    #magnitudes = np.sqrt((mag_ns**2 + mag_ew**2) / 2)\n",
    "    # Combine horizontal magnitudes with vector magnitude formula:\n",
    "    magnitudes = np.sqrt((mag_ns**2) + (mag_ew**2))\n",
    "    # or change to preferred method of your choosing\n",
    "\n",
    "    # Phases:\n",
    "    # phases_ns = np.angle(fft_ns) \n",
    "    # phases_ew = np.angle(fft_ew)\n",
    "\n",
    "    # Apply smoothing if requested:\n",
    "    if smooth:\n",
    "        magnitudes = smooth_spectrum(magnitudes, window_size=smooth_window)\n",
    "    \n",
    "    # Create figures:\n",
    "    if fig1 is None:\n",
    "        fig1 = plt.figure(figsize=(10,6))\n",
    "        plt.plot(freqs, magnitudes, linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (Linear)', fontsize=16)\n",
    "        plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        plt.minorticks_on()\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    else:\n",
    "        plt.figure(fig1.number)\n",
    "        plt.plot(freqs, magnitudes, linewidth=1, label=name)\n",
    "        if name:\n",
    "            plt.legend(loc='upper right')\n",
    "    \n",
    "    if fig2 is None:\n",
    "        fig2 = plt.figure(figsize=(10, 6))\n",
    "        plt.loglog(freqs[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.title('FFT Magnitude Spectrum (log)', fontsize=16)\n",
    "        plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "        plt.ylabel('Magnitude', fontsize=14)\n",
    "        plt.xticks(fontsize=14)\n",
    "        plt.yticks(fontsize=14)\n",
    "        plt.grid(True, which='major', alpha=0.5)\n",
    "        plt.grid(True, which='minor', alpha=0.5)\n",
    "        plt.tight_layout()\n",
    "        if name:\n",
    "            plt.legend(loc='lower left')\n",
    "    else:\n",
    "        plt.figure(fig2.number)\n",
    "        plt.loglog(freqs[1:], magnitudes[1:], linewidth=1, label=name)\n",
    "        plt.legend()\n",
    "        if name:\n",
    "            plt.legend(loc='lower left')\n",
    "    \n",
    "    #if fig3 is None:\n",
    "     #   fig3 = plt.figure(figsize=(10, 6))\n",
    "      #  plt.plot(freqs[1:], phases[1:], linewidth=1, label=name)\n",
    "       # plt.title('Phase Spectrum', fontsize=16)\n",
    "        #plt.xlabel('Frequency (Hz)', fontsize=14)\n",
    "        #plt.ylabel('Phase (radians)', fontsize=14)\n",
    "        #plt.grid(True, alpha=0.5)\n",
    "        #plt.xticks(fontsize=14)\n",
    "        #plt.yticks(fontsize=14)\n",
    "        #plt.xlim(-1, 50)\n",
    "        #plt.tight_layout()\n",
    "        #if name:\n",
    "         #   plt.legend(loc='lower right', framealpha=0.5)\n",
    "    #else:\n",
    "     #   plt.figure(fig3.number)\n",
    "      #  plt.plot(freqs[1:], phases[1:], linewidth=1, label=name)\n",
    "       # if name:\n",
    "        #    plt.legend(loc='lower right', framealpha=0.8)\n",
    "    \n",
    "    return freqs, magnitudes, fig1, fig2#, fig3   Leaving phases out for now\n",
    "\n",
    "print(\"horizontal FFT for dataframe: df\")\n",
    "# First call: Full data - creates new figures\n",
    "freqs_full, mags_full, fig1, fig2 = plot_fft_horizontal(df, 'NS_Normalized_Shifted', df, 'EW_Normalized_Shifted', \n",
    "    sr=original_sampling_rate, name='Full Horizontal Data', apply_window=False, smooth=True, smooth_window=5)\n",
    "\n",
    "# Second call: Subsampled/tapered data - reuses the same figures\n",
    "NS_sub_results_df_dframe = pd.DataFrame(NS_sub_results_df, columns=['Subsampled NS from df'])\n",
    "EW_sub_results_df_dframe = pd.DataFrame(EW_sub_results_df, columns=['Subsampled EW from df'])\n",
    "freqs_sub, mags_sub, fig1, fig2 = plot_fft_horizontal(NS_sub_results_df_dframe, 'Subsampled NS from df', \n",
    "                                EW_sub_results_df_dframe, 'Subsampled EW from df', sr=new_sr_df,\n",
    "    fig1=fig1,  # Pass the existing figures\n",
    "    fig2=fig2, name='Subsampled/Tapered Horizontal Data', apply_window=True, smooth=True, smooth_window=5)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42aab4c6-cfbf-48cf-8924-739e242092ac",
   "metadata": {},
   "source": [
    "***\n",
    "### FFT With Moving Window\n",
    "##### Notes:\n",
    "- Goal: apply moving window (20 second intervals)\n",
    "- Using short time fourier transform from scipy.signal\n",
    "- Issue with previous version of code: time was not the same dimensions as the STFT result\n",
    "    - Possible solutions: Plot spectrogram to show frequency content overtime, or plot average magnitude spectrum\n",
    "- Tried log scale for spectrogram frequency axis, did not display well for most full data plots\n",
    "- adjust y-axis limits to focus on areas of interest"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26267b18-ad6d-4aba-bd0f-ea1511ac1969",
   "metadata": {},
   "source": [
    "#### Spectrograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5649ea3e-f0a2-4027-8afe-d3647dacbf22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ShortTimeFFT(win, hop, fs, *, fft_mode='onesided', mfft=None, dual_win=None, scale_to=None, phase_shift=0)\n",
    "# win = window function\n",
    "# hop = time increment in signal samples for sliding window\n",
    "# fs = sampling frequency of output signal and window\n",
    "# fft_mode = Mode of FFT to be used (default onesided)\n",
    "# mfft = Length of the FFT used, if a zero padded FFT is desired. If None (default), the length of the window win is used\n",
    "\n",
    "# Problem with previous code (deleted): time not same dimensions as STFT result\n",
    "# Possible solutions:\n",
    "# Method 1: spectrogram\n",
    "def hann_windowed_fft_spectrogram(data, sr, plot_title, vmax, vmin, window_duration=20):\n",
    "    \"\"\"Compute FFT with scipy short time fourier transform\n",
    "    data: Time series to perform STFT on (array)\n",
    "    sr: sampling rate\n",
    "    plot_title: title for spectrogram plot (string)\n",
    "    window_duration: window duration in seconds\"\"\"\n",
    "    hop = int(sr*window_duration) # hop = number of samples in window_duration (20sec)\n",
    "    \n",
    "    # Generate hann window array with tapering only at edges\n",
    "    window_array = np.ones(hop)  # Start with flat window\n",
    "    taper_length = int(hop * 0.1)  # 0.1 for tapering at 10% from edges\n",
    "    \n",
    "    # Apply taper to left edge\n",
    "    left_window = np.hanning(2*taper_length)[:taper_length]\n",
    "    window_array[:taper_length] = left_window\n",
    "    \n",
    "    # Apply taper to right edge\n",
    "    right_window = np.hanning(2*taper_length)[taper_length:]\n",
    "    window_array[-taper_length:] = right_window\n",
    "    \n",
    "    STF = ShortTimeFFT(win=window_array, hop=hop, fs=sr, fft_mode='onesided', mfft=None, scale_to='magnitude', phase_shift=0)\n",
    "    STF_result = STF.stft(data)\n",
    "    \n",
    "    # Get time and frequency arrays\n",
    "    time_axis = STF.t(len(data))  # Time values for each window (sec)\n",
    "    time_minute = time_axis/60\n",
    "    freq_axis = STF.f  # Frequency values\n",
    "    \n",
    "    # Plot spectrogram\n",
    "    stft_plot = plt.figure(figsize=(12,4))\n",
    "    plt.pcolormesh(time_minute, freq_axis, np.abs(STF_result), cmap='binary', shading='gouraud', vmax=vmax, vmin=vmin)\n",
    "    plt.title(plot_title)\n",
    "    plt.xlabel(\"Time (min)\")\n",
    "    plt.ylabel(\"Frequency (Hz)\")\n",
    "    #plt.yscale('log')\n",
    "    cbar=plt.colorbar(label='Magnitude')\n",
    "    cbar.formatter.set_powerlimits((0, 0))\n",
    "    plt.grid(which='major')\n",
    "    plt.grid(which='minor')\n",
    "    \n",
    "    return stft_plot\n",
    "\n",
    "for i, dframe in enumerate(dataframes):\n",
    "    if i==0:\n",
    "        print(\"dataframe: df\")\n",
    "    else:\n",
    "        print(f\"dataframe: df{i+1}\")\n",
    "    # Plot!------------------------------------------------------------\n",
    "    # Spectrograms for NS data:\n",
    "    print(\"Spectrograms for North-South data\")\n",
    "    # Full NS data:\n",
    "    hann_windowed_fft_spectrogram(dframe['NS_Normalized_Shifted'].values, sr=original_sampling_rate, \n",
    "                             plot_title='Short Time Fourier Transform With 20s Hann Window (Full NS Data)', vmax=0.001, vmin=0, window_duration=20)\n",
    "    plt.ylim(top=500)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Tapered/subsampled NS data:\n",
    "    hann_windowed_fft_spectrogram(NS_taper_sub[i], sr=new_sr_df, plot_title='Short Time Fourier Transform With 20s Hann Window (Tapered/Subsampled NS Data)',\n",
    "                             vmax=0.001, vmin=0, window_duration=20) \n",
    "    # new sampling rate should be the same for all dataframes because they have been subsampled the same\n",
    "    plt.ylim(top=70)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #---------------------------------------------------------------------------------\n",
    "    # Spectrograms for EW data:\n",
    "    print(\"Spectrograms for East-West data\")\n",
    "    # Full EW data:\n",
    "    hann_windowed_fft_spectrogram(dframe['EW_Normalized_Shifted'].values, sr=original_sampling_rate, \n",
    "                             plot_title='Short Time Fourier Transform With 20s Hann Window (Full EW Data)', vmax=0.001, vmin=0, window_duration=20)\n",
    "    plt.ylim(top=500)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Tapered/subsampled EW data:\n",
    "    hann_windowed_fft_spectrogram(EW_taper_sub[i], sr=new_sr_df, plot_title='Short Time Fourier Transform With 20s Hann Window (Tapered/Subsampled EW Data)',\n",
    "                             vmax=0.001, vmin=0, window_duration=20)\n",
    "    plt.ylim(top=70)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    #--------------------------------------------------------------------------------\n",
    "    # Spectrograms for Z-component data\n",
    "    print(\"Spectrograms for Z-component data\")\n",
    "    # Full Z data:\n",
    "    hann_windowed_fft_spectrogram(dframe['Z_Normalized_Shifted'].values, sr=original_sampling_rate, \n",
    "                             plot_title='Short Time Fourier Transform With 20s Hann Window (Full Z Data)', vmax=0.001, vmin=0, window_duration=20)\n",
    "    plt.ylim(top=500)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    # Tapered/subsampled Z data:\n",
    "    hann_windowed_fft_spectrogram(Z_taper_sub[i], sr=new_sr_df, plot_title='Short Time Fourier Transform With 20s Hann Window (Tapered/Subsampled Z Data)',\n",
    "                             vmax=0.001, vmin=0, window_duration=20)\n",
    "    plt.ylim(top=70)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    print(\"~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca4993d-f562-441b-a35a-9a326552d023",
   "metadata": {},
   "source": [
    "***\n",
    "### H/V Ratio\n",
    "#### Notes:\n",
    "- Divide horizontal magnitude by vertical magnitude to get H/V ratio\n",
    "- Used vector-magnitude formula to combine horizontal magnitudes for North-South and East-West components"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f731f5ce-4485-4788-8e65-a02e60c9dc9c",
   "metadata": {},
   "source": [
    "#### Filter for Smoothing\n",
    "- applied Savitzky-Golay filter for smoothing data\n",
    "    - also tried an envelope function for smoothing using the Hilbert transform\n",
    "- limited displayed frequency range"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5993c53-509f-4950-ab28-ce72a0a6e501",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Adding envelope for smoothing using Hilbert transform:\n",
    "def envelope(input_data):\n",
    "    \"\"\"Compute envelope of input_data using Hilbert transform\"\"\"\n",
    "    signal= hilbert(input_data)\n",
    "    return np.abs(signal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3afab90c-6692-409f-a9a3-5645ee4a743a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# original w updated tapering function:\n",
    "# (Note: this version is shown in your updated slides)\n",
    "def HV_ratio(data_ns, column_ns, data_ew, column_ew, data_z, column_z, sr, apply_window=True, freq_min=0.1, freq_max=None):\n",
    "    \"\"\"Compute Horizontal-to-Vertical Spectral Ratio (HVSR)\n",
    "    data_ns: North-South component (array)\n",
    "    data_ew: East-West component (array)\n",
    "    data_z: Vertical (Z) component (array)\n",
    "    sr: Sampling rate\n",
    "    freq_min: Minimum frequency to include (Hz)\n",
    "    freq_max: Maximum frequency to include (Hz), default is 80% of Nyquist\"\"\"\n",
    "    n = len(data_ns)\n",
    "    \n",
    "    if freq_max is None:\n",
    "        freq_max = 0.8*(sr/2)  # 80% of Nyquist to avoid edge effects\n",
    "    \n",
    "    # Apply window if requested\n",
    "    if apply_window:\n",
    "        windowed_data_ns = apply_hann_window1(data_ns, column_ns, taper_fraction=0.1).values\n",
    "        windowed_data_ew = apply_hann_window1(data_ew, column_ew, taper_fraction=0.1).values\n",
    "        windowed_data_z = apply_hann_window1(data_z, column_z, taper_fraction=0.1).values\n",
    "        \n",
    "        coherent_gain = calculate_coherent_gain(data_ns[column_ns], taper_fraction=0.1)\n",
    "    else:\n",
    "        windowed_data_ns = data_ns[column_ns].values\n",
    "        windowed_data_ew = data_ew[column_ew].values\n",
    "        windowed_data_z = data_z[column_z].values\n",
    "        coherent_gain = 1.0\n",
    "    \n",
    "    # Compute FFTs\n",
    "    fft_ns = np.fft.rfft(windowed_data_ns)\n",
    "    fft_ew = np.fft.rfft(windowed_data_ew)\n",
    "    fft_z = np.fft.rfft(windowed_data_z)\n",
    "    \n",
    "    freqs = np.fft.rfftfreq(n, d=1/sr)\n",
    "    \n",
    "    # Compute magnitudes\n",
    "    mag_ns = np.abs(fft_ns) / n\n",
    "    mag_ew = np.abs(fft_ew) / n\n",
    "    mag_z = np.abs(fft_z) / n\n",
    "    \n",
    "    # Double AC components for one-sided spectrum correction\n",
    "    mag_ns[1:] *= 2.0\n",
    "    mag_ew[1:] *= 2.0\n",
    "    mag_z[1:] *= 2.0\n",
    "    \n",
    "    if n % 2 == 0:\n",
    "        mag_ns[-1] /= 2.0\n",
    "        mag_ew[-1] /= 2.0\n",
    "        mag_z[-1] /= 2.0\n",
    "    \n",
    "    # Compute horizontal RMS\n",
    "    # mag_horizontal = np.sqrt((mag_ns**2 + mag_ew**2) / 2)\n",
    "    # Use vector-magnitude formula to combine horizontal magnitudes:\n",
    "    mag_horizontal = np.sqrt((mag_ns**2) + (mag_ew**2))\n",
    "    \n",
    "    # Compute HVSR with threshold to avoid division by tiny numbers\n",
    "    z_threshold = np.max(mag_z) * 1e-8\n",
    "    hvsr = np.zeros_like(mag_horizontal)\n",
    "    mask = mag_z > z_threshold\n",
    "    hvsr[mask] = mag_horizontal[mask] / mag_z[mask]\n",
    "    \n",
    "    # Clip extreme values?\n",
    "    #hvsr = np.clip(hvsr, 0, clip_max) # comment out when don't want to use\n",
    "    \n",
    "    # Apply frequency range mask\n",
    "    freq_mask = (freqs >= freq_min) & (freqs <= freq_max)\n",
    "    \n",
    "    return freqs[freq_mask], hvsr[freq_mask]\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "print(\"dataframe: df\")\n",
    "# Compute HVSR for full data (df)\n",
    "freqs1, hvsr1 = HV_ratio(df, 'NS_Normalized_Shifted', df, 'EW_Normalized_Shifted', df, 'Z_Normalized_Shifted',\n",
    "        sr=original_sampling_rate, apply_window=False, freq_min=0.1, freq_max=300)  # Or use None for auto freq_max\n",
    "        #clip_max=10)\n",
    "\n",
    "# Compute HVSR for tapered/subsampled data (df)\n",
    "Z_sub_results_df_dframe = pd.DataFrame(Z_sub_results_df, columns=['Subsampled Z from df'])\n",
    "freqs2, hvsr2 = HV_ratio(NS_sub_results_df_dframe, 'Subsampled NS from df', EW_sub_results_df_dframe, 'Subsampled EW from df',\n",
    "        Z_sub_results_df_dframe, 'Subsampled Z from df', sr=new_sr_df, apply_window=True, freq_min=0.1,\n",
    "        freq_max=70)  # Should be less than decimated Nyquist (~73 Hz for subsampling step=7)\n",
    "        #clip_max=10)\n",
    "\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Apply smoothing with Savitzky-Golay filter\n",
    "hvsr1_smooth = savgol_filter(hvsr1, window_length=20481, polyorder=2) ## For 20s window, 20s*(sampling rate in /s) = number of samples in 20 sec\n",
    "# Note: window length should be an odd integer to allow the filter window to be centered symmetrically on the current data point\n",
    "hvsr2_smooth = savgol_filter(hvsr2, window_length=2925, polyorder=2) # 20*new_sr_df = approx. 2925\n",
    "#~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# Try second pass through Savitzky-Golay filter instead of envelope:\n",
    "hvsr1_smooth = savgol_filter(hvsr1_smooth, window_length=20001, polyorder=2)\n",
    "hvsr2_smooth = savgol_filter(hvsr2_smooth, window_length=10001, polyorder=2)\n",
    "# third pass?\n",
    "hvsr1_smooth = savgol_filter(hvsr1_smooth, window_length=10001, polyorder=2)\n",
    "hvsr2_smooth = savgol_filter(hvsr2_smooth, window_length=10001, polyorder=2)\n",
    "hvsr2_smooth = savgol_filter(hvsr2_smooth, window_length=10001, polyorder=2) # 4th pass for tapered/subsampled data\n",
    "hvsr2_smooth = savgol_filter(hvsr2_smooth, window_length=10001, polyorder=2) # 5th pass for tapered/subsampled data\n",
    "\n",
    "########## Testing envelope with Hilbert transform ################################################################################\n",
    "# call envelope function from previous cell:\n",
    "#hvsr1_smooth = envelope(hvsr1_smooth)\n",
    "#hvsr2_smooth = envelope(hvsr2_smooth)\n",
    "# doesn't work well, adds spikes at edges of plot\n",
    "###################################################################################################################################################\n",
    "# Find main peak (full data)\n",
    "peak_idx = np.argmax(hvsr1_smooth[(freqs1 > 10) & (freqs1 < 50)])\n",
    "peak_freq = freqs1[(freqs1 > 10) & (freqs1 < 50)][peak_idx]\n",
    "peak_value = hvsr1_smooth[(freqs1 > 10) & (freqs1 < 50)][peak_idx]\n",
    "print(f\"Full data peak frequency: {peak_freq} Hz\")\n",
    "print(f\"Full data peak H/V: {peak_value}\")\n",
    "# (tapered/subsampled data):\n",
    "peak_idx2 = np.argmax(hvsr2_smooth[(freqs2 > 10) & (freqs2 < 50)])\n",
    "peak_freq2 = freqs2[(freqs2 > 10) & (freqs2 < 50)][peak_idx2]\n",
    "peak_value2 = hvsr2_smooth[(freqs2 > 10) & (freqs2 < 50)][peak_idx2]\n",
    "print(f\"Tapered/Subsampled data peak frequency: {peak_freq2} Hz\")\n",
    "print(f\"Tapered/Subsampled data peak H/V: {peak_value2}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12, 6))\n",
    "# Smoothed data\n",
    "plt.semilogx(freqs1, hvsr1_smooth, label=\"H/V From Full Data\")\n",
    "plt.semilogx(freqs2, hvsr2_smooth, label=\"H/V From Tapered/Subsampled Data\")\n",
    "plt.axhline(y=1, color='grey', linestyle='--', alpha=0.5, label='H/V = 1')\n",
    "plt.xlabel('Frequency (Hz)')\n",
    "plt.ylabel('H/V Ratio')\n",
    "plt.title('Smoothed Horizontal-to-Vertical Spectral Ratio (HVSR) - Ver.1')\n",
    "plt.ylim([0, 5])  # Adjust based on your data\n",
    "plt.grid(True, which=\"both\", alpha=0.5)\n",
    "plt.axvline(x=30.63, linestyle=\"--\", color='green', label='30.63 Hz')\n",
    "plt.legend(loc='upper left')\n",
    "plt.show()\n",
    "print(\"Note: 30.63 +/- 3.81 Hz is the max. H/V from screenshot of Grilla average H/V plot\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "504f130b-4a64-46a5-b950-340fb084d39b",
   "metadata": {},
   "source": [
    "#### Interpreting Horizontal-to-Vertical Spectral Ratios\n",
    "- Peaks should occur at fundamental and higher order resonance frequencies\n",
    "    - (fundamental frequency = lowest resonant frequency in a system)\n",
    "- Typically, the fundamental resonance frequency is the first/lowest value indicated by the highest amplitude peak on the H/V frequency spectrum\n",
    "- H/V = 1 indicates that the horizontal and vertical motions are equal\n",
    "- H/V < 1 indicates that horizontal motion is weaker than vertical motion\n",
    "- H/V > 1 indicates that horizontal motion is stronger than vertical motion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "184d6bd7-adc7-43e9-a73e-4962736f4657",
   "metadata": {},
   "source": [
    "### Plotting H/V Ratio From Grilla .txt File:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4933f5f3-8cc5-47c8-b8ff-6a7576d64801",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Trying to use H/V .txt file downloaded from Grilla:\n",
    "column_names = ['freq.', 'H/V']\n",
    "dtf = pd.read_table('Tyendinaga_HV_ratio.asc', delimiter=r'\\s+', encoding='latin-1', skiprows=31, names=column_names, usecols=column_names)\n",
    "#dtf.head().style"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d65e686-39f9-415f-84f3-b6351780620b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(dtf['freq.'], dtf['H/V'], label=\"H/V Ratio from Grilla Data File\")\n",
    "plt.xlabel(\"Frequency (Hz)\")\n",
    "plt.ylabel(\"H/V Ratio\")\n",
    "plt.title(\"H/V Ratio from Grilla Data File\")\n",
    "plt.axvline(x=30.63, color='red', linestyle='--', label=\"30.63 Hz\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.5)\n",
    "plt.xscale('log')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfed7ba-754c-4e90-a7aa-2f8a6817598c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove unneeded columns: NS, EW, Z, nsL, ewL, zL, aY, aX, aZ, NS_Detrended, EW_Detrended, Z_Detrended, NS_Normalized, EW_Normalized, Z_Normalized\n",
    "# Will be left with DataFrame containing time (s) and the Normalized_Shifted data for NS, EW, and Z components\n",
    "df_updated = df.drop(['NS', 'EW', 'Z', 'nsL', 'ewL', 'zL', 'aY', 'aX', 'aZ', 'NS_Detrended', 'EW_Detrended', 'Z_Detrended', 'NS_Normalized', \n",
    "                      'EW_Normalized', 'Z_Normalized'], axis=1)\n",
    "# df_updated.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2af19e9e-dffb-481a-9f46-e5dab39190e0",
   "metadata": {},
   "source": [
    "### Convert Cell Outputs to PDF:\n",
    "- can currently download as .html file, which when opened in a browser can be saved as a pdf using print (Ctrl+P) -> save to pdf <br>\n",
    "    - using code to directly download as .pdf file is not working currently\n",
    "    - if opened with Jupyter Notebook, can save full file as .pdf directly by going to File -> Save and Export Notebook As -> PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "332c8401-5953-46c0-8e49-3ac4034543fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save notebook as .html file\n",
    "def save_notebook(notebook_name, output_name=None):\n",
    "    \"\"\"Save Jupyter notebook outputs and markdown to .html file (excludes code cells)\n",
    "    notebook_name : str, name of the notebook file (e.g., 'notebook.ipynb')\n",
    "    output_name : str, name for the output file (optional). If None, uses the notebook name with .html extension\"\"\"\n",
    "    \n",
    "    # Set output file name\n",
    "    if output_name is None:\n",
    "        output_name = notebook_name.replace('.ipynb', '.html')\n",
    "    \n",
    "    # Check if notebook file exists\n",
    "    if not os.path.exists(notebook_name):\n",
    "        raise FileNotFoundError(f\"Notebook file '{notebook_name}' not found\")\n",
    "            \n",
    "    result = subprocess.run(['jupyter', 'nbconvert', '--to', 'html', '--no-input',  # Exclude code cells\n",
    "    notebook_name, '--output', output_name.replace('.html', '')], capture_output=True, text=True)\n",
    "            \n",
    "    if result.returncode == 0: # return code of zero indicates successful execution \n",
    "        print(f\"Created HTML file: {output_name}\")\n",
    "        print(\"Open this in your browser and use Ctrl+P --> Save as PDF\")\n",
    "        print(\"(The HTML file contains only outputs and markdown, no code cells)\")\n",
    "    else:\n",
    "        print(\"Error:\")\n",
    "        print(result.stderr) # standard error\n",
    "\n",
    "# Example usage:\n",
    "# save_notebook('notebook.ipynb')\n",
    "# Or specify custom output name:\n",
    "# save_notebook('notebook.ipynb', 'output_notebook.html')\n",
    "save_notebook('Proj_2_File_1_Tyendinaga_Updated-Copy1.ipynb', 'Proj_2_File_1_Tyendinaga_Updated-Copy1.html')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
